{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-12T09:11:58.899038Z",
     "iopub.status.busy": "2025-07-12T09:11:58.898171Z",
     "iopub.status.idle": "2025-07-12T09:13:11.935427Z",
     "shell.execute_reply": "2025-07-12T09:13:11.934731Z",
     "shell.execute_reply.started": "2025-07-12T09:11:58.899012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 17511, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 17511 (delta 5), reused 0 (delta 0), pack-reused 17493 (from 3)\u001b[K\n",
      "Receiving objects: 100% (17511/17511), 16.65 MiB | 31.29 MiB/s, done.\n",
      "Resolving deltas: 100% (11996/11996), done.\n",
      "/kaggle/working/yolov5\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%cd yolov5 \n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:14:55.740068Z",
     "iopub.status.busy": "2025-07-12T09:14:55.739349Z",
     "iopub.status.idle": "2025-07-12T09:14:59.338701Z",
     "shell.execute_reply": "2025-07-12T09:14:59.337998Z",
     "shell.execute_reply.started": "2025-07-12T09:14:55.740036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your paths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:15:01.455764Z",
     "iopub.status.busy": "2025-07-12T09:15:01.455385Z",
     "iopub.status.idle": "2025-07-12T09:15:01.460215Z",
     "shell.execute_reply": "2025-07-12T09:15:01.459600Z",
     "shell.execute_reply.started": "2025-07-12T09:15:01.455738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights_path = \"/kaggle/input/yolov5m/pytorch/default/1/best.pt\"\n",
    "images_path = \"/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/images\"\n",
    "videos_path = \"/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/videos\"\n",
    "project_path = \"/kaggle/working\"                                # Project root directory\n",
    "device = \"0,1\"                                                  # The device on which to run detection task\n",
    "submission_filename = \"submission_detection_tracking_RGB.csv\"\n",
    "\n",
    "devices = [\"0,1\", \"cpu\"]                                        # Tracking task will run on gpu and cpu both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:15:04.361466Z",
     "iopub.status.busy": "2025-07-12T09:15:04.360737Z",
     "iopub.status.idle": "2025-07-12T09:15:04.703149Z",
     "shell.execute_reply": "2025-07-12T09:15:04.702233Z",
     "shell.execute_reply.started": "2025-07-12T09:15:04.361442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/yolov5m/pytorch/default/1/best.pt\n",
      "/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/images\n",
      "/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/videos\n"
     ]
    }
   ],
   "source": [
    "!echo \"$weights_path\"\n",
    "!echo \"$images_path\"\n",
    "!echo \"$videos_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:15:08.457715Z",
     "iopub.status.busy": "2025-07-12T09:15:08.457011Z",
     "iopub.status.idle": "2025-07-12T09:15:08.471791Z",
     "shell.execute_reply": "2025-07-12T09:15:08.471047Z",
     "shell.execute_reply.started": "2025-07-12T09:15:08.457684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "detect = r\"\"\"\n",
    "# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from ultralytics.utils.plotting import Annotator, colors, save_one_box\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from utils.general import (\n",
    "    LOGGER,\n",
    "    Profile,\n",
    "    check_file,\n",
    "    check_img_size,\n",
    "    check_imshow,\n",
    "    check_requirements,\n",
    "    colorstr,\n",
    "    cv2,\n",
    "    increment_path,\n",
    "    non_max_suppression,\n",
    "    print_args,\n",
    "    scale_boxes,\n",
    "    strip_optimizer,\n",
    "    xyxy2xywh,\n",
    ")\n",
    "from utils.torch_utils import select_device, smart_inference_mode\n",
    "\n",
    "\n",
    "def box_iou2(boxes1, boxes2):\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # (N, M, 2)\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # (N, M, 2)\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # (N, M, 2)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # (N, M)\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def remove_enclosing_boxes_vectorized(preds, area_thresh=1.05):\n",
    "    if preds.shape[0] <= 1:\n",
    "        return preds\n",
    "\n",
    "    boxes = preds[:, :4]  # (N, 4)\n",
    "    confs = preds[:, 4]   # (N,)\n",
    "    labels = preds[:, 5]  # (N,)\n",
    "    N = preds.shape[0]\n",
    "\n",
    "    x1 = boxes[:, 0].unsqueeze(1)\n",
    "    y1 = boxes[:, 1].unsqueeze(1)\n",
    "    x2 = boxes[:, 2].unsqueeze(1)\n",
    "    y2 = boxes[:, 3].unsqueeze(1)\n",
    "\n",
    "    area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])  # (N,)\n",
    "\n",
    "    # Broadcast comparisons\n",
    "    encloses = (\n",
    "        (x1 <= x1.T) & (y1 <= y1.T) &\n",
    "        (x2 >= x2.T) & (y2 >= y2.T)\n",
    "    )  # (N, N)\n",
    "\n",
    "    larger_area = area.unsqueeze(1) > area.unsqueeze(0) * area_thresh\n",
    "    same_class = labels.unsqueeze(1) == labels.unsqueeze(0)\n",
    "    higher_conf = confs.unsqueeze(1) < confs.unsqueeze(0)\n",
    "\n",
    "    # i encloses j AND i is larger AND same class AND j has higher conf â†’ suppress i\n",
    "    suppress = encloses & larger_area & same_class & higher_conf\n",
    "\n",
    "    to_keep = ~(suppress.any(dim=1))  # Keep boxes not suppressed by any better one\n",
    "\n",
    "    return preds[to_keep]\n",
    "\n",
    "\n",
    "def temporal_completion(filtered_all, iou_thresh=0.4, window=3):\n",
    "    for i in range(len(filtered_all)):\n",
    "        current = filtered_all[i]\n",
    "        if current.shape[0] > 0:\n",
    "            continue  # already has detections\n",
    "\n",
    "        to_add = []\n",
    "        for offset in range(1, window + 1):\n",
    "            prev_idx = i - offset\n",
    "            next_idx = i + offset\n",
    "            if prev_idx < 0 or next_idx >= len(filtered_all):\n",
    "                continue\n",
    "\n",
    "            prev = filtered_all[prev_idx]\n",
    "            nxt = filtered_all[next_idx]\n",
    "            if prev.shape[0] == 0 or nxt.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            ious = box_iou2(prev[:, :4], nxt[:, :4])\n",
    "            class_match = prev[:, None, 5] == nxt[:, None, 5].T\n",
    "            ious[~class_match] = 0.0\n",
    "            matched = (ious >= iou_thresh)\n",
    "\n",
    "            for p_idx, n_idx in torch.nonzero(matched, as_tuple=False):\n",
    "                box1 = prev[p_idx]\n",
    "                box2 = nxt[n_idx]\n",
    "                interpolated_box = (box1[:4] + box2[:4]) / 2\n",
    "                interpolated_conf = min(box1[4], box2[4]) * 0.9  # slightly lower confidence\n",
    "                interpolated_label = box1[5]\n",
    "                new_det = torch.cat([interpolated_box, interpolated_conf.unsqueeze(0), interpolated_label.unsqueeze(0)])\n",
    "                to_add.append(new_det.unsqueeze(0))\n",
    "\n",
    "        if to_add:\n",
    "            filled = torch.cat(to_add, dim=0)\n",
    "            filled = remove_enclosing_boxes_vectorized(filled)\n",
    "            filtered_all[i] = filled\n",
    "    \n",
    "    return filtered_all\n",
    "\n",
    "\n",
    "max_label_dist = 0\n",
    "gt_included_count = 0\n",
    "def temporal_filter_preds(all_preds, iou_thresh=0.4, window=3, boost_conf=0.1, min_matches=1):\n",
    "    filtered_all = []\n",
    "    max_label_iou = 0\n",
    "\n",
    "    def convert_labels_to_xyxy(labels):\n",
    "        cx, cy, w, h = labels[:, 1], labels[:, 2], labels[:, 3], labels[:, 4]\n",
    "        x1, y1 = cx - w / 2, cy - h / 2\n",
    "        x2, y2 = cx + w / 2, cy + h / 2\n",
    "        return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "    def get_pred_boxes(preds):\n",
    "        return preds[:, :4]\n",
    "\n",
    "    for i, preds in enumerate(all_preds):\n",
    "        if preds.shape[0] == 0:\n",
    "            filtered_all.append(preds)\n",
    "            continue\n",
    "\n",
    "        pred_boxes = get_pred_boxes(preds)  # (N, 4)\n",
    "        match_counts = torch.zeros(preds.shape[0], device=preds.device)\n",
    "\n",
    "        for offset in range(-window, window + 1):\n",
    "            if offset == 0:\n",
    "                continue\n",
    "            j = i + offset\n",
    "            if 0 <= j < len(all_preds):\n",
    "                neighbor_preds = all_preds[j]\n",
    "                if neighbor_preds.shape[0] == 0:\n",
    "                    continue\n",
    "                neighbor_boxes = get_pred_boxes(neighbor_preds)\n",
    "\n",
    "                ious = box_iou2(pred_boxes, neighbor_boxes)  # (N, M)\n",
    "                class_match = preds[:, None, 5] == neighbor_preds[None, :, 5]\n",
    "                ious[~class_match] = 0.0  # force non-class matches to zero IoU\n",
    "                match_counts += (ious >= iou_thresh).any(dim=1).float()\n",
    "\n",
    "        keep_mask = match_counts >= min_matches\n",
    "        kept = preds[keep_mask]\n",
    "\n",
    "        boost_mask = match_counts[keep_mask] >= window * 2\n",
    "        if boost_mask.any():\n",
    "            kept[boost_mask, 4] = torch.clamp(kept[boost_mask, 4] + boost_conf, 0.0, 1.0)\n",
    "\n",
    "        kept = remove_enclosing_boxes_vectorized(kept)\n",
    "        filtered_all.append(kept)\n",
    "    return filtered_all\n",
    "\n",
    "\n",
    "@smart_inference_mode()\n",
    "def run(\n",
    "    weights=ROOT / \"yolov5s.pt\",  # model path or triton URL\n",
    "    source=ROOT / \"data/images\",  # file/dir/URL/glob/screen/0(webcam)\n",
    "    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n",
    "    imgsz=(640, 640),  # inference size (height, width)\n",
    "    conf_thres=0.25,  # confidence threshold\n",
    "    iou_thres=0.45,  # NMS IOU threshold\n",
    "    max_det=1000,  # maximum detections per image\n",
    "    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "    view_img=False,  # show results\n",
    "    save_txt=False,  # save results to *.txt\n",
    "    save_format=0,  # save boxes coordinates in YOLO format or Pascal-VOC format (0 for YOLO and 1 for Pascal-VOC)\n",
    "    save_csv=False,  # save results in CSV format\n",
    "    save_conf=False,  # save confidences in --save-txt labels\n",
    "    save_crop=False,  # save cropped prediction boxes\n",
    "    nosave=False,  # do not save images/videos\n",
    "    classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
    "    agnostic_nms=False,  # class-agnostic NMS\n",
    "    augment=False,  # augmented inference\n",
    "    visualize=False,  # visualize features\n",
    "    update=False,  # update all models\n",
    "    project=ROOT / \"runs/detect\",  # save results to project/name\n",
    "    name=\"exp\",  # save results to project/name\n",
    "    exist_ok=False,  # existing project/name ok, do not increment\n",
    "    line_thickness=3,  # bounding box thickness (pixels)\n",
    "    hide_labels=False,  # hide labels\n",
    "    hide_conf=False,  # hide confidences\n",
    "    half=False,  # use FP16 half-precision inference\n",
    "    dnn=False,  # use OpenCV DNN for ONNX inference\n",
    "    vid_stride=1,  # video frame-rate stride\n",
    "):\n",
    "    \n",
    "    source = str(source)\n",
    "    save_img = not nosave and not source.endswith(\".txt\")  # save inference images\n",
    "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
    "    is_url = source.lower().startswith((\"rtsp://\", \"rtmp://\", \"http://\", \"https://\"))\n",
    "    webcam = source.isnumeric() or source.endswith(\".streams\") or (is_url and not is_file)\n",
    "    screenshot = source.lower().startswith(\"screen\")\n",
    "    if is_url and is_file:\n",
    "        source = check_file(source)  # download\n",
    "\n",
    "    # Directories\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "    (save_dir / \"labels\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Load model\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
    "    stride, names, pt = model.stride, model.names, model.pt\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "    # Dataloader\n",
    "    bs = 1  # batch_size\n",
    "    if webcam:\n",
    "        view_img = check_imshow(warn=True)\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "        bs = len(dataset)\n",
    "    elif screenshot:\n",
    "        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "    # Run inference\n",
    "    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
    "    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n",
    "    for p in dt:\n",
    "        p.dt = 0.0\n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        # print(im0s.shape)\n",
    "        for p in dt:\n",
    "            p.dt = 0.0\n",
    "        with dt[0]:\n",
    "            im = torch.from_numpy(im).to(model.device)\n",
    "            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "            if model.xml and im.shape[0] > 1:\n",
    "                ims = torch.chunk(im, im.shape[0], 0)\n",
    "\n",
    "        # Inference\n",
    "        with dt[1]:\n",
    "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "            if model.xml and im.shape[0] > 1:\n",
    "                pred = None\n",
    "                for image in ims:\n",
    "                    if pred is None:\n",
    "                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n",
    "                    else:\n",
    "                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n",
    "                pred = [pred, None]\n",
    "            else:\n",
    "                pred = model(im, augment=augment, visualize=visualize)\n",
    "        # NMS\n",
    "        with dt[2]:\n",
    "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "\n",
    "        # pred = temporal_filter_preds(pred)\n",
    "        # pred = temporal_completion(pred)\n",
    "        # Second-stage classifier (optional)\n",
    "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
    "\n",
    "        # Define the path for the CSV file\n",
    "        csv_path = save_dir / \"predictions.csv\"\n",
    "\n",
    "        # Create or append to the CSV file\n",
    "        def write_to_csv(image_name, prediction, confidence):\n",
    "            data = {\"Image Name\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n",
    "            file_exists = os.path.isfile(csv_path)\n",
    "            with open(csv_path, mode=\"a\", newline=\"\") as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=data.keys())\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "                writer.writerow(data)\n",
    "\n",
    "        # Process predictions\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            seen += 1\n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "                s += f\"{i}: \"\n",
    "            else:\n",
    "                p, im0, frame = path, im0s.copy(), getattr(dataset, \"frame\", 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # im.jpg\n",
    "            txt_path = str(save_dir / \"labels\" / p.stem) + (\"\" if dataset.mode == \"image\" else f\"_{frame}\")  # im.txt\n",
    "            s += \"{:g}x{:g} \".format(*im.shape[2:])  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            # print(\"gain: \" + str(gn))\n",
    "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, 5].unique():\n",
    "                    n = (det[:, 5] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    c = int(cls)  # integer class\n",
    "                    label = names[c] if hide_conf else f\"{names[c]}\"\n",
    "                    confidence = float(conf)\n",
    "                    confidence_str = f\"{confidence:.2f}\"\n",
    "                    total_time_ms = sum(p.dt for p in dt) * 1000\n",
    "\n",
    "                    if save_csv:\n",
    "                        write_to_csv(p.name, label, confidence_str)\n",
    "\n",
    "                    if save_txt:  # Write to file\n",
    "                        if save_format == 0:\n",
    "                            coords = (\n",
    "                                (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                            )  # normalized xywh\n",
    "                        else:\n",
    "                            coords = (torch.tensor(xyxy).view(1, 4) / gn).view(-1).tolist()  # xyxy\n",
    "                        # print(coords)\n",
    "                        line = (cls, *coords, conf, total_time_ms) if save_conf else (cls, *coords, total_time_ms)  # label format\n",
    "                        with open(f\"{txt_path}.txt\", \"a\") as f:\n",
    "                            f.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n",
    "\n",
    "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                        c = int(cls)  # integer class\n",
    "                        label = None if hide_labels else (names[c] if hide_conf else f\"{names[c]} {conf:.2f}\")\n",
    "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                    if save_crop:\n",
    "                        save_one_box(xyxy, imc, file=save_dir / \"crops\" / names[c] / f\"{p.stem}.jpg\", BGR=True)\n",
    "\n",
    "            # Stream results\n",
    "            im0 = annotator.result()\n",
    "            if view_img:\n",
    "                if platform.system() == \"Linux\" and p not in windows:\n",
    "                    windows.append(p)\n",
    "                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
    "                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n",
    "                cv2.imshow(str(p), im0)\n",
    "                cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == \"image\":\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path[i] != save_path:  # new video\n",
    "                        vid_path[i] = save_path\n",
    "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                            vid_writer[i].release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                        save_path = str(Path(save_path).with_suffix(\".mp4\"))  # force *.mp4 suffix on results videos\n",
    "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "                    vid_writer[i].write(im0)\n",
    "\n",
    "        # Print time (inference-only)\n",
    "        # LOGGER.info(f\"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1e3:.1f}ms\")\n",
    "\n",
    "    # Print results\n",
    "    #t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n",
    "    #LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}\" % t)\n",
    "    if save_txt or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else \"\"\n",
    "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "    if update:\n",
    "        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s.pt\", help=\"model path or triton URL\")\n",
    "    parser.add_argument(\"--source\", type=str, default=ROOT / \"data/images\", help=\"file/dir/URL/glob/screen/0(webcam)\")\n",
    "    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"(optional) dataset.yaml path\")\n",
    "    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", nargs=\"+\", type=int, default=[640], help=\"inference size h,w\")\n",
    "    parser.add_argument(\"--conf-thres\", type=float, default=0.25, help=\"confidence threshold\")\n",
    "    parser.add_argument(\"--iou-thres\", type=float, default=0.45, help=\"NMS IoU threshold\")\n",
    "    parser.add_argument(\"--max-det\", type=int, default=1000, help=\"maximum detections per image\")\n",
    "    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n",
    "    parser.add_argument(\"--view-img\", action=\"store_true\", help=\"show results\")\n",
    "    parser.add_argument(\"--save-txt\", action=\"store_true\", help=\"save results to *.txt\")\n",
    "    parser.add_argument(\n",
    "        \"--save-format\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"whether to save boxes coordinates in YOLO format or Pascal-VOC format when save-txt is True, 0 for YOLO and 1 for Pascal-VOC\",\n",
    "    )\n",
    "    parser.add_argument(\"--save-csv\", action=\"store_true\", help=\"save results in CSV format\")\n",
    "    parser.add_argument(\"--save-conf\", action=\"store_true\", help=\"save confidences in --save-txt labels\")\n",
    "    parser.add_argument(\"--save-crop\", action=\"store_true\", help=\"save cropped prediction boxes\")\n",
    "    parser.add_argument(\"--nosave\", action=\"store_true\", help=\"do not save images/videos\")\n",
    "    parser.add_argument(\"--classes\", nargs=\"+\", type=int, help=\"filter by class: --classes 0, or --classes 0 2 3\")\n",
    "    parser.add_argument(\"--agnostic-nms\", action=\"store_true\", help=\"class-agnostic NMS\")\n",
    "    parser.add_argument(\"--augment\", action=\"store_true\", help=\"augmented inference\")\n",
    "    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"visualize features\")\n",
    "    parser.add_argument(\"--update\", action=\"store_true\", help=\"update all models\")\n",
    "    parser.add_argument(\"--project\", default=ROOT / \"runs/detect\", help=\"save results to project/name\")\n",
    "    parser.add_argument(\"--name\", default=\"exp\", help=\"save results to project/name\")\n",
    "    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n",
    "    parser.add_argument(\"--line-thickness\", default=3, type=int, help=\"bounding box thickness (pixels)\")\n",
    "    parser.add_argument(\"--hide-labels\", default=False, action=\"store_true\", help=\"hide labels\")\n",
    "    parser.add_argument(\"--hide-conf\", default=False, action=\"store_true\", help=\"hide confidences\")\n",
    "    parser.add_argument(\"--half\", action=\"store_true\", help=\"use FP16 half-precision inference\")\n",
    "    parser.add_argument(\"--dnn\", action=\"store_true\", help=\"use OpenCV DNN for ONNX inference\")\n",
    "    parser.add_argument(\"--vid-stride\", type=int, default=1, help=\"video frame-rate stride\")\n",
    "    opt = parser.parse_args()\n",
    "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
    "    print_args(vars(opt))\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    \n",
    "    check_requirements(ROOT / \"requirements.txt\", exclude=(\"tensorboard\", \"thop\"))\n",
    "    run(**vars(opt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"detect2.py\", 'w') as f:\n",
    "    f.write(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:15:15.781551Z",
     "iopub.status.busy": "2025-07-12T09:15:15.781080Z",
     "iopub.status.idle": "2025-07-12T09:18:34.982538Z",
     "shell.execute_reply": "2025-07-12T09:18:34.981622Z",
     "shell.execute_reply.started": "2025-07-12T09:15:15.781525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[34m\u001b[1mdetect2: \u001b[0mweights=['/kaggle/input/yolov5m/pytorch/default/1/best.pt'], source=/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.2, iou_thres=0.45, max_det=1000, device=0,1, view_img=False, save_txt=True, save_format=1, save_csv=True, save_conf=True, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ğŸš€ v7.0-422-g2540fd4c Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "                                                              CUDA:1 (Tesla T4, 15095MiB)\n",
      "\n",
      "Fusing layers... \n",
      "p2_cfg summary: 212 layers, 20856975 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n",
      "6123 labels saved to runs/detect/exp/labels\n"
     ]
    }
   ],
   "source": [
    "!python detect2.py --weights \"$weights_path\" --source \"$images_path\" --conf-thres 0.2 --save-format 1 --device \"$device\" --save-txt --save-csv --save-conf --nosave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:18:44.894479Z",
     "iopub.status.busy": "2025-07-12T09:18:44.893805Z",
     "iopub.status.idle": "2025-07-12T09:18:44.905262Z",
     "shell.execute_reply": "2025-07-12T09:18:44.904750Z",
     "shell.execute_reply.started": "2025-07-12T09:18:44.894450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "obj = r\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils.general import set_logging\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, Profile, check_file, check_img_size, \n",
    "                            check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args,\n",
    "                            scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0] \n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT)) \n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd())) \n",
    "\n",
    "#---------------Object Tracking---------------\n",
    "import skimage\n",
    "import supervision as sv\n",
    "\n",
    "#-----------Object Blurring-------------------\n",
    "blurratio = 40\n",
    "\n",
    "\n",
    "#.................. Tracker Functions .................\n",
    "'''Computer Color for every box and track'''\n",
    "palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "def compute_color_for_labels(label):\n",
    "    color = [int(int(p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "    return tuple(color)\n",
    "\n",
    "\n",
    "def bbox_rel(*xyxy):\n",
    "    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n",
    "    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n",
    "    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n",
    "    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n",
    "    x_c = (bbox_left + bbox_w / 2)\n",
    "    y_c = (bbox_top + bbox_h / 2)\n",
    "    w = bbox_w\n",
    "    h = bbox_h\n",
    "    return x_c, y_c, w, h\n",
    "\n",
    "\n",
    "def draw_boxes(img, bbox, identities=None, categories=None, \n",
    "                names=None, color_box=None,offset=(0, 0)):\n",
    "    for i, box in enumerate(bbox):\n",
    "        x1, y1, x2, y2 = [int(i) for i in box]\n",
    "        x1 += offset[0]\n",
    "        x2 += offset[0]\n",
    "        y1 += offset[1]\n",
    "        y2 += offset[1]\n",
    "        cat = int(categories[i]) if categories is not None else 0\n",
    "        id = int(identities[i]) if identities is not None else 0\n",
    "        data = (int((box[0]+box[2])/2),(int((box[1]+box[3])/2)))\n",
    "        label = str(id)\n",
    "\n",
    "        if color_box:\n",
    "            color = compute_color_for_labels(id)\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2),color, 2)\n",
    "            cv2.rectangle(img, (x1, y1 - 20), (x1 + w, y1), (255,191,0), -1)\n",
    "            cv2.putText(img, label, (x1, y1 - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "            [255, 255, 255], 1)\n",
    "            cv2.circle(img, data, 3, color,-1)\n",
    "        else:\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2),(255,191,0), 2)\n",
    "            cv2.rectangle(img, (x1, y1 - 20), (x1 + w, y1), (255,191,0), -1)\n",
    "            cv2.putText(img, label, (x1, y1 - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "            [255, 255, 255], 1)\n",
    "            cv2.circle(img, data, 3, (255,191,0),-1)\n",
    "    return img\n",
    "#..............................................................................\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def detect(weights=ROOT / 'yolov5n.pt',\n",
    "        source=ROOT / 'yolov5/data/images', \n",
    "        data=ROOT / 'yolov5/data/coco128.yaml',  \n",
    "        imgsz=(640, 640),conf_thres=0.25,iou_thres=0.45,  \n",
    "        max_det=1000, device='cpu',  view_img=False,  \n",
    "        save_txt=False, save_conf=False, save_crop=False, \n",
    "        nosave=False, classes=None,  agnostic_nms=False,  \n",
    "        augment=False, visualize=False,  update=False,  \n",
    "        project=ROOT / 'runs/detect',  name='exp',  \n",
    "        exist_ok=False, line_thickness=2,hide_labels=False,  \n",
    "        hide_conf=False,half=False,dnn=False,display_labels=False,\n",
    "        blur_obj=False,color_box = False,):\n",
    "    \n",
    "    save_img = not nosave and not source.endswith('.txt') \n",
    "    \n",
    "    #.... Initialize SORT .... \n",
    "    \n",
    "    track_color_id = 0\n",
    "\n",
    "    byte_tracker = sv.ByteTrack()\n",
    "    smoother = sv.DetectionsSmoother()\n",
    "    label = sv.LabelAnnotator()\n",
    "    trace_annotator = sv.TraceAnnotator()\n",
    "    video_info = sv.VideoInfo.from_video_path(source)\n",
    "    frame_generator = sv.get_video_frames_generator(source)\n",
    "    fps_monitor = sv.FPSMonitor()\n",
    "    #......................... \n",
    "    \n",
    "    \n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  \n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "    set_logging()\n",
    "    device = select_device(device)\n",
    "    half &= device.type != 'cpu'  \n",
    "\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n",
    "    stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
    "    imgsz = check_img_size(imgsz, s=stride)  \n",
    "\n",
    "    half &= (pt or jit or onnx or engine) and device.type != 'cpu'  \n",
    "    if pt or jit:\n",
    "        model.model.half() if half else model.model.float()\n",
    "\n",
    "    if webcam:\n",
    "        cudnn.benchmark = True  \n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = len(dataset) \n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = 1 \n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "    \n",
    "    with sv.VideoSink(target_path=\"output.mp4\", video_info=video_info) as sink:\n",
    "        for path, im, im0s, vid_cap, s in dataset:\n",
    "\n",
    "            svframe = next(frame_generator)\n",
    "            t1 = time_sync()\n",
    "            im = torch.from_numpy(im).to(device)\n",
    "            im = im.half() if half else im.float()  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "            t2 = time_sync()\n",
    "            dt[0] += t2 - t1\n",
    "    \n",
    "            # Inference\n",
    "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "            pred = model(im, augment=augment, visualize=visualize)\n",
    "            t3 = time_sync()\n",
    "            dt[1] += t3 - t2\n",
    "    \n",
    "            # NMS\n",
    "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "            dt[2] += time_sync() - t3\n",
    "    \n",
    "            \n",
    "            for i, det in enumerate(pred):  # per image\n",
    "                seen += 1\n",
    "                if webcam:  # batch_size >= 1\n",
    "                    p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "                    s += f'{i}: '\n",
    "                else:\n",
    "                    p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "    \n",
    "                p = Path(p)\n",
    "                save_path = str(save_dir / p.name)\n",
    "                txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "                s += '%gx%g ' % im.shape[2:]\n",
    "                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
    "                # print(gn)\n",
    "                imc = im0.copy() if save_crop else im0\n",
    "                annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "                if len(det):\n",
    "                    det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "                    for c in det[:, -1].unique():\n",
    "                        n = (det[:, -1] == c).sum()\n",
    "                        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"\n",
    "    \n",
    "                    # Write results\n",
    "                    for *xyxy, conf, cls in reversed(det):\n",
    "                        if blur_obj:\n",
    "                            crop_obj = im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])]\n",
    "                            blur = cv2.blur(crop_obj,(blurratio,blurratio))\n",
    "                            im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])] = blur\n",
    "                        else:\n",
    "                            continue\n",
    "                    #..................USE TRACK FUNCTION....................\n",
    "                    \n",
    "                    \n",
    "                    # Run bytetrack\n",
    "                    pred_np = det.cpu().numpy()\n",
    "                    xyxy = pred_np[:, :4]\n",
    "                    confidence = pred_np[:, 4]\n",
    "                    class_id = pred_np[:, 5].astype(int)\n",
    "                    detections = sv.Detections(\n",
    "                        xyxy=xyxy,\n",
    "                        confidence=confidence,\n",
    "                        class_id=class_id\n",
    "                    )\n",
    "                    detections = byte_tracker.update_with_detections(detections)\n",
    "                    detections = smoother.update_with_detections(detections)\n",
    "                    # print(\"Track_id: \" + str(detections.tracker_id) + \", track_confidence: \" + str(detections.confidence))\n",
    "                    labels = [ f\"{tracker_id}, {confidence:.2f}\" for tracker_id, confidence in zip(detections.tracker_id, detections.confidence) ]\n",
    "                    annotated_frame = label.annotate(scene=svframe.copy(), detections=detections, labels=labels)\n",
    "                    annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "                    sink.write_frame(frame=annotated_frame)\n",
    "                    \n",
    "                        \n",
    "                    if save_txt:\n",
    "                        os.makedirs(save_dir / \"labels\", exist_ok=True)\n",
    "                        frame_txt_path = f\"{save_dir / 'labels' / p.stem}_{frame}.txt\"\n",
    "                        with open(frame_txt_path, 'w') as f:\n",
    "                            for box, tracker_id, class_id, track_conf, det_conf in zip(\n",
    "                                    detections.xyxy, detections.tracker_id, detections.class_id, detections.confidence, confidence):\n",
    "                                coords = (torch.tensor(box).view(1, 4) / gn).view(-1).tolist()  # xyxy\n",
    "                                x1, y1, x2, y2 = coords\n",
    "                                line = f\"{int(tracker_id)} {int(class_id)} {track_conf:.2f} {det_conf:.2f} {x1:.4f} {y1:.4f} {x2:.4f} {y2:.4f}\\n\"\n",
    "                                f.write(line)\n",
    "\n",
    "\n",
    "    \n",
    "                if view_img:\n",
    "                    cv2.imshow(str(p), im0)\n",
    "                    cv2.waitKey(1) \n",
    "                if save_img:\n",
    "                    if dataset.mode == 'image':\n",
    "                        cv2.imwrite(save_path, im0)\n",
    "                    else:\n",
    "                        if vid_path != save_path: \n",
    "                            vid_path = save_path\n",
    "                            if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                                vid_writer.release()  \n",
    "                            if vid_cap: \n",
    "                                fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                                w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                                h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                            else:\n",
    "                                fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                                save_path += '.mp4'\n",
    "                            vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                        vid_writer.write(im0)\n",
    "            #print(\"Frame Processing!\")\n",
    "            fps_monitor.tick()\n",
    "    print(\"Video Exported Success\")\n",
    "    print(str(fps_monitor.fps))\n",
    "\n",
    "    if update:\n",
    "        strip_optimizer(weights)\n",
    "    \n",
    "    if vid_cap:\n",
    "        vid_cap.release()\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
    "    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')\n",
    "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
    "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--view-img', action='store_true', help='show results')\n",
    "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
    "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
    "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n",
    "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "    parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
    "    parser.add_argument('--update', action='store_true', help='update all models')\n",
    "    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n",
    "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
    "    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
    "    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
    "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
    "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
    "    parser.add_argument('--blur-obj', action='store_true', help='Blur Detected Objects')\n",
    "    parser.add_argument('--color-box', action='store_true', help='Change color of every box and track')\n",
    "    opt = parser.parse_args()\n",
    "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
    "    print_args(vars(opt))\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    check_requirements(exclude=('tensorboard', 'thop'))\n",
    "    detect(**vars(opt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n",
    "\"\"\"\n",
    "\n",
    "with open('obj_det_and_trk2.py', 'w') as f:\n",
    "    f.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:19:27.839414Z",
     "iopub.status.busy": "2025-07-12T09:19:27.838669Z",
     "iopub.status.idle": "2025-07-12T09:28:57.793469Z",
     "shell.execute_reply": "2025-07-12T09:28:57.792752Z",
     "shell.execute_reply.started": "2025-07-12T09:19:27.839388Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: V_DRONE_099\n",
      "Processing video: V_BIRD_043\n",
      "Processing video: V_BIRD_050\n",
      "Processing video: V_BIRD_051\n",
      "Processing video: V_BIRD_045\n",
      "Processing video: V_DRONE_111\n",
      "Processing video: V_DRONE_104\n",
      "Processing video: V_DRONE_101\n",
      "Processing video: V_BIRD_047\n",
      "Processing video: V_DRONE_114\n",
      "Processing video: V_DRONE_105\n",
      "Processing video: V_DRONE_108\n",
      "{'V_DRONE_099': {'0,1': 63.334480201126695, 'cpu': 10.443430662878828}, 'V_BIRD_043': {'0,1': 66.6382666221393, 'cpu': 11.100440188067248}, 'V_BIRD_050': {'0,1': 64.50725269557364, 'cpu': 10.024978349672631}, 'V_BIRD_051': {'0,1': 70.56630212515533, 'cpu': 11.744176697646747}, 'V_BIRD_045': {'0,1': 68.40427089177041, 'cpu': 8.444900102603455}, 'V_DRONE_111': {'0,1': 71.14050995320736, 'cpu': 11.485375813396452}, 'V_DRONE_104': {'0,1': 59.58089440431222, 'cpu': 9.54690013533739}, 'V_DRONE_101': {'0,1': 63.17631029877135, 'cpu': 10.854303300071493}, 'V_BIRD_047': {'0,1': 62.53915133281106, 'cpu': 9.130903722004303}, 'V_DRONE_114': {'0,1': 69.90211788176335, 'cpu': 11.077041961872741}, 'V_DRONE_105': {'0,1': 70.80808640526563, 'cpu': 11.699907518315754}, 'V_DRONE_108': {'0,1': 62.1550424924421, 'cpu': 11.002610352435308}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "conf_thres = 0.01\n",
    "\n",
    "fps_results = {}\n",
    "\n",
    "videos = glob.glob(os.path.join(videos_path, \"*.mp4\"))\n",
    "\n",
    "for video in videos:\n",
    "    video_name = os.path.splitext(os.path.basename(video))[0]\n",
    "    fps_results[video_name] = {}\n",
    "    print(\"Processing video: \" + video_name)\n",
    "\n",
    "    for device in devices:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"obj_det_and_trk2.py\", \"--weights\", weights_path, \"--source\",  video, \"--imgsz\", \"320\", \"--conf-thres\", str(conf_thres), \"--save-txt\", \"--save-conf\", \"--project\", project_path, \"--device\", device],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            fps_value = float(result.stdout.strip().split('\\n')[-1])\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting FPS for {video} on {device}: {e}\")\n",
    "            fps_value = -1\n",
    "\n",
    "        fps_results[video_name][device] = fps_value\n",
    "\n",
    "# Optional: Print or save the results\n",
    "print(fps_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !python obj_det_and_trk2.py --weights /kaggle/input/yolov5m/pytorch/default/1/best.pt --source /kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/videos/V_DRONE_101.mp4 --imgsz 320 --conf-thres 0.01 --save-txt --save-conf --project=/kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:29:04.230860Z",
     "iopub.status.busy": "2025-07-12T09:29:04.230306Z",
     "iopub.status.idle": "2025-07-12T09:29:05.109005Z",
     "shell.execute_reply": "2025-07-12T09:29:05.108370Z",
     "shell.execute_reply.started": "2025-07-12T09:29:04.230834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 exp directories.\n",
      "Processing label directory: /kaggle/working/exp/labels\n",
      "Updated all label files in /kaggle/working/exp/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp2/labels\n",
      "Updated all label files in /kaggle/working/exp2/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp3/labels\n",
      "Updated all label files in /kaggle/working/exp3/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp4/labels\n",
      "Updated all label files in /kaggle/working/exp4/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp5/labels\n",
      "Updated all label files in /kaggle/working/exp5/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp6/labels\n",
      "Updated all label files in /kaggle/working/exp6/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp7/labels\n",
      "Updated all label files in /kaggle/working/exp7/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp8/labels\n",
      "Updated all label files in /kaggle/working/exp8/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp9/labels\n",
      "Updated all label files in /kaggle/working/exp9/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp10/labels\n",
      "Updated all label files in /kaggle/working/exp10/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp11/labels\n",
      "Updated all label files in /kaggle/working/exp11/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp12/labels\n",
      "Updated all label files in /kaggle/working/exp12/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp13/labels\n",
      "Updated all label files in /kaggle/working/exp13/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp14/labels\n",
      "Updated all label files in /kaggle/working/exp14/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp15/labels\n",
      "Updated all label files in /kaggle/working/exp15/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp16/labels\n",
      "Updated all label files in /kaggle/working/exp16/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp17/labels\n",
      "Updated all label files in /kaggle/working/exp17/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp18/labels\n",
      "Updated all label files in /kaggle/working/exp18/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp19/labels\n",
      "Updated all label files in /kaggle/working/exp19/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp20/labels\n",
      "Updated all label files in /kaggle/working/exp20/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp21/labels\n",
      "Updated all label files in /kaggle/working/exp21/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp22/labels\n",
      "Updated all label files in /kaggle/working/exp22/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp23/labels\n",
      "Updated all label files in /kaggle/working/exp23/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp24/labels\n",
      "Updated all label files in /kaggle/working/exp24/labels with trend column.\n",
      "All exp directories processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = \"/kaggle/working\"\n",
    "exp_dirs = sorted(glob.glob(os.path.join(base_dir, \"exp*\")), key=os.path.getmtime)\n",
    "\n",
    "print(f\"Found {len(exp_dirs)} exp directories.\")\n",
    "\n",
    "for exp_dir in exp_dirs:\n",
    "    labels_dir = os.path.join(exp_dir, \"labels\")\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        print(f\"Skipping {exp_dir}, no 'labels' folder found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing label directory: {labels_dir}\")\n",
    "\n",
    "    # Collect bounding box area per track_id per frame\n",
    "    track_areas = defaultdict(list)  # track_id: list of (frame_number, area)\n",
    "    file_list = sorted(glob.glob(os.path.join(labels_dir, \"*.txt\")))\n",
    "    frame_index_map = {file: idx for idx, file in enumerate(file_list)}\n",
    "\n",
    "    for txt_file in file_list:\n",
    "        frame_id = frame_index_map[txt_file]\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "                track_id = int(parts[0])\n",
    "                x1, y1, x2, y2 = map(float, parts[4:8])\n",
    "                area = (x2 - x1) * (y2 - y1) * 320 * 256\n",
    "                track_areas[track_id].append((frame_id, area))\n",
    "\n",
    "    # Compute trend per track_id\n",
    "    track_trend = {}\n",
    "    for track_id, entries in track_areas.items():\n",
    "        entries.sort()\n",
    "        frame_ids, areas = zip(*entries)\n",
    "        if len(areas) < 3:\n",
    "            trend = \"stationary\"\n",
    "        else:\n",
    "            slope = np.polyfit(frame_ids, areas, 1)[0]\n",
    "            if slope > 0.0:\n",
    "                trend = \"approaching\"\n",
    "            else:\n",
    "                trend = \"receding\"\n",
    "        track_trend[track_id] = trend\n",
    "\n",
    "    # Rewrite each txt file with trend info as final column\n",
    "    for txt_file in file_list:\n",
    "        new_lines = []\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 8:\n",
    "                    new_lines.append(line)\n",
    "                    continue\n",
    "                track_id = int(parts[0])\n",
    "                trend = track_trend.get(track_id, \"stationary\")\n",
    "                new_line = line.strip() + f\" {trend}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        with open(txt_file, \"w\") as f:\n",
    "            f.writelines(new_lines)\n",
    "\n",
    "    print(f\"Updated all label files in {labels_dir} with trend column.\")\n",
    "\n",
    "print(\"All exp directories processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T08:20:57.085759Z",
     "iopub.status.busy": "2025-07-12T08:20:57.085500Z",
     "iopub.status.idle": "2025-07-12T08:20:57.091202Z",
     "shell.execute_reply": "2025-07-12T08:20:57.090401Z",
     "shell.execute_reply.started": "2025-07-12T08:20:57.085738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T09:29:16.641028Z",
     "iopub.status.busy": "2025-07-12T09:29:16.640753Z",
     "iopub.status.idle": "2025-07-12T09:29:16.652764Z",
     "shell.execute_reply": "2025-07-12T09:29:16.652047Z",
     "shell.execute_reply.started": "2025-07-12T09:29:16.641009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV saved to submission_detection_tracking_RGB.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "submission_rows = []\n",
    "\n",
    "# ---------- PART 1: Image Detection Labels ----------\n",
    "detection_dir = os.path.join(\"yolov5\", \"runs\", \"detect\", \"exp\", \"labels\")\n",
    "for txt_file in sorted(glob.glob(os.path.join(detection_dir, \"*.txt\"))):\n",
    "    frame_name = os.path.basename(txt_file).replace(\".txt\", \"\")\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 7:\n",
    "                continue\n",
    "            class_id, conf_det, x_min, y_min, x_max, y_max, infer_time = map(float, parts)\n",
    "            class_label = \"bird\" if int(class_id) == 0 else \"drone\"\n",
    "            row = [\n",
    "                frame_name,\n",
    "                0,\n",
    "                x_min, y_min, x_max, y_max,\n",
    "                class_label,\n",
    "                \"0\",\n",
    "                round(conf_det, 2),\n",
    "                round(infer_time, 2),\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0\n",
    "            ]\n",
    "            submission_rows.append(row)\n",
    "\n",
    "# ---------- PART 2: Video Tracking Labels ----------\n",
    "tracking_base_dir = \".\"\n",
    "exp_dirs = sorted([d for d in os.listdir(tracking_base_dir) if d.startswith(\"exp\") and os.path.isdir(os.path.join(tracking_base_dir, d))])\n",
    "\n",
    "for i, exp in enumerate(exp_dirs):\n",
    "    labels_dir = os.path.join(tracking_base_dir, exp, \"labels\")\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        continue\n",
    "\n",
    "    # Determine device by exp index (odd = GPU, even = CPU)\n",
    "    if i % 2 == 0:\n",
    "        continue\n",
    "\n",
    "    for txt_file in sorted(glob.glob(os.path.join(labels_dir, \"*.txt\"))):\n",
    "        frame_name = os.path.basename(txt_file).replace(\".txt\", \"\")\n",
    "        video_name = \"_\".join(frame_name.split(\"_\")[:3])\n",
    "\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                track_id, class_id, conf_track, conf_det, x_min, y_min, x_max, y_max, direction = parts\n",
    "                class_label = \"bird\" if int(class_id) == 0 else \"drone\"\n",
    "                row = [\n",
    "                    frame_name,\n",
    "                    int(track_id),\n",
    "                    float(x_min), float(y_min), float(x_max), float(y_max),\n",
    "                    class_label,\n",
    "                    direction,\n",
    "                    round(float(conf_det), 2),\n",
    "                    0,\n",
    "                    round(fps_results.get(video_name, {}).get(\"cpu\", 0), 2),\n",
    "                    round(fps_results.get(video_name, {}).get(\"0,1\", 0), 2),\n",
    "                    round(float(conf_track), 2),\n",
    "                    0,\n",
    "                    0\n",
    "                ]\n",
    "                submission_rows.append(row)\n",
    "\n",
    "# ---------- PART 3: Write Final CSV ----------\n",
    "csv_path = submission_filename\n",
    "header = [\n",
    "    \"Frame_name\", \"track_id\", \"x_min_norm\", \"y_min_norm\", \"x_max_norm\", \"y_max_norm\",\n",
    "    \"class_label\", \"direction\", \"confidence_detection\", \"inference_time_detection (ms)\",\n",
    "    \"FPS (CPU)\", \"FPS (GPU)\", \"confidence_track\", \"payload_label\", \"prob_harmful\"\n",
    "]\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(submission_rows)\n",
    "\n",
    "print(f\"Submission CSV saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:38:22.608608Z",
     "iopub.status.busy": "2025-07-11T10:38:22.608068Z",
     "iopub.status.idle": "2025-07-11T10:38:22.740995Z",
     "shell.execute_reply": "2025-07-11T10:38:22.740209Z",
     "shell.execute_reply.started": "2025-07-11T10:38:22.608583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r -q labels.zip /kaggle/working/exp8/labels"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7749164,
     "sourceId": 12294869,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 367558,
     "modelInstanceId": 346285,
     "sourceId": 424855,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
