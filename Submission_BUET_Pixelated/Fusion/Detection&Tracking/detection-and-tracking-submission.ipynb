{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-12T16:25:06.933954Z",
     "iopub.status.busy": "2025-07-12T16:25:06.933337Z",
     "iopub.status.idle": "2025-07-12T16:26:18.342082Z",
     "shell.execute_reply": "2025-07-12T16:26:18.341213Z",
     "shell.execute_reply.started": "2025-07-12T16:25:06.933924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 17511, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 17511 (delta 5), reused 0 (delta 0), pack-reused 17493 (from 3)\u001b[K\n",
      "Receiving objects: 100% (17511/17511), 16.62 MiB | 30.33 MiB/s, done.\n",
      "Resolving deltas: 100% (12000/12000), done.\n",
      "/kaggle/working/yolov5\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%cd yolov5 \n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:26:18.344092Z",
     "iopub.status.busy": "2025-07-12T16:26:18.343840Z",
     "iopub.status.idle": "2025-07-12T16:26:24.948567Z",
     "shell.execute_reply": "2025-07-12T16:26:24.947850Z",
     "shell.execute_reply.started": "2025-07-12T16:26:18.344067Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define your paths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:39:22.013489Z",
     "iopub.status.busy": "2025-07-12T16:39:22.012941Z",
     "iopub.status.idle": "2025-07-12T16:39:22.017524Z",
     "shell.execute_reply": "2025-07-12T16:39:22.016870Z",
     "shell.execute_reply.started": "2025-07-12T16:39:22.013462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights_path = \"/kaggle/input/yolov5m_fused/pytorch/default/1/yolov5m_fusion.pt\"\n",
    "images_path_RGB = \"/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/images\"\n",
    "videos_path_RGB = \"/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/videos\"\n",
    "images_path_IR = \"/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/IR/images\"\n",
    "videos_path_IR = \"/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/IR/videos\"\n",
    "project_path = \"/kaggle/working\"                                # Project root directory\n",
    "device = \"0,1\"                                                  # The device on which to run detection task\n",
    "submission_filename = \"submission_detection_tracking_fused.csv\"\n",
    "\n",
    "devices = [\"0,1\", \"cpu\"]                                        # Tracking task will run on gpu and cpu both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:26:24.956765Z",
     "iopub.status.busy": "2025-07-12T16:26:24.956564Z",
     "iopub.status.idle": "2025-07-12T16:26:24.974077Z",
     "shell.execute_reply": "2025-07-12T16:26:24.973450Z",
     "shell.execute_reply.started": "2025-07-12T16:26:24.956749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "detect = r\"\"\"\n",
    "# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from ultralytics.utils.plotting import Annotator, colors, save_one_box\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from utils.general import (\n",
    "    LOGGER,\n",
    "    Profile,\n",
    "    check_file,\n",
    "    check_img_size,\n",
    "    check_imshow,\n",
    "    check_requirements,\n",
    "    colorstr,\n",
    "    cv2,\n",
    "    increment_path,\n",
    "    non_max_suppression,\n",
    "    print_args,\n",
    "    scale_boxes,\n",
    "    strip_optimizer,\n",
    "    xyxy2xywh,\n",
    ")\n",
    "from utils.torch_utils import select_device, smart_inference_mode\n",
    "\n",
    "\n",
    "def box_iou2(boxes1, boxes2):\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # (N, M, 2)\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # (N, M, 2)\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # (N, M, 2)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # (N, M)\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def remove_enclosing_boxes_vectorized(preds, area_thresh=1.05):\n",
    "    if preds.shape[0] <= 1:\n",
    "        return preds\n",
    "\n",
    "    boxes = preds[:, :4]  # (N, 4)\n",
    "    confs = preds[:, 4]   # (N,)\n",
    "    labels = preds[:, 5]  # (N,)\n",
    "    N = preds.shape[0]\n",
    "\n",
    "    x1 = boxes[:, 0].unsqueeze(1)\n",
    "    y1 = boxes[:, 1].unsqueeze(1)\n",
    "    x2 = boxes[:, 2].unsqueeze(1)\n",
    "    y2 = boxes[:, 3].unsqueeze(1)\n",
    "\n",
    "    area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])  # (N,)\n",
    "\n",
    "    # Broadcast comparisons\n",
    "    encloses = (\n",
    "        (x1 <= x1.T) & (y1 <= y1.T) &\n",
    "        (x2 >= x2.T) & (y2 >= y2.T)\n",
    "    )  # (N, N)\n",
    "\n",
    "    larger_area = area.unsqueeze(1) > area.unsqueeze(0) * area_thresh\n",
    "    same_class = labels.unsqueeze(1) == labels.unsqueeze(0)\n",
    "    higher_conf = confs.unsqueeze(1) < confs.unsqueeze(0)\n",
    "\n",
    "    # i encloses j AND i is larger AND same class AND j has higher conf â†’ suppress i\n",
    "    suppress = encloses & larger_area & same_class & higher_conf\n",
    "\n",
    "    to_keep = ~(suppress.any(dim=1))  # Keep boxes not suppressed by any better one\n",
    "\n",
    "    return preds[to_keep]\n",
    "\n",
    "\n",
    "def temporal_completion(filtered_all, iou_thresh=0.65, window=2):\n",
    "    for i in range(len(filtered_all)):\n",
    "        current = filtered_all[i]\n",
    "        if current.shape[0] > 0:\n",
    "            continue  # already has detections\n",
    "\n",
    "        to_add = []\n",
    "        for offset in range(1, window + 1):\n",
    "            prev_idx = i - offset\n",
    "            next_idx = i + offset\n",
    "            if prev_idx < 0 or next_idx >= len(filtered_all):\n",
    "                continue\n",
    "\n",
    "            prev = filtered_all[prev_idx]\n",
    "            nxt = filtered_all[next_idx]\n",
    "            if prev.shape[0] == 0 or nxt.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            ious = box_iou2(prev[:, :4], nxt[:, :4])\n",
    "            class_match = prev[:, None, 5] == nxt[:, None, 5].T\n",
    "            ious[~class_match] = 0.0\n",
    "            matched = (ious >= iou_thresh)\n",
    "\n",
    "            for p_idx, n_idx in torch.nonzero(matched, as_tuple=False):\n",
    "                box1 = prev[p_idx]\n",
    "                box2 = nxt[n_idx]\n",
    "                interpolated_box = (box1[:4] + box2[:4]) / 2\n",
    "                interpolated_conf = min(box1[4], box2[4]) * 0.9  # slightly lower confidence\n",
    "                interpolated_label = box1[5]\n",
    "                new_det = torch.cat([interpolated_box, interpolated_conf.unsqueeze(0), interpolated_label.unsqueeze(0)])\n",
    "                to_add.append(new_det.unsqueeze(0))\n",
    "\n",
    "        if to_add:\n",
    "            filled = torch.cat(to_add, dim=0)\n",
    "            filled = remove_enclosing_boxes_vectorized(filled)\n",
    "            filtered_all[i] = filled\n",
    "    \n",
    "    return filtered_all\n",
    "\n",
    "\n",
    "max_label_dist = 0\n",
    "gt_included_count = 0\n",
    "def temporal_filter_preds(all_preds, iou_thresh=0.65, window=2, boost_conf=0.1, min_matches=1):\n",
    "    filtered_all = []\n",
    "    max_label_iou = 0\n",
    "\n",
    "    def convert_labels_to_xyxy(labels):\n",
    "        cx, cy, w, h = labels[:, 1], labels[:, 2], labels[:, 3], labels[:, 4]\n",
    "        x1, y1 = cx - w / 2, cy - h / 2\n",
    "        x2, y2 = cx + w / 2, cy + h / 2\n",
    "        return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "    def get_pred_boxes(preds):\n",
    "        return preds[:, :4]\n",
    "\n",
    "    for i, preds in enumerate(all_preds):\n",
    "        if preds.shape[0] == 0:\n",
    "            filtered_all.append(preds)\n",
    "            continue\n",
    "\n",
    "        pred_boxes = get_pred_boxes(preds)  # (N, 4)\n",
    "        match_counts = torch.zeros(preds.shape[0], device=preds.device)\n",
    "\n",
    "        for offset in range(-window, window + 1):\n",
    "            if offset == 0:\n",
    "                continue\n",
    "            j = i + offset\n",
    "            if 0 <= j < len(all_preds):\n",
    "                neighbor_preds = all_preds[j]\n",
    "                if neighbor_preds.shape[0] == 0:\n",
    "                    continue\n",
    "                neighbor_boxes = get_pred_boxes(neighbor_preds)\n",
    "\n",
    "                ious = box_iou2(pred_boxes, neighbor_boxes)  # (N, M)\n",
    "                class_match = preds[:, None, 5] == neighbor_preds[None, :, 5]\n",
    "                ious[~class_match] = 0.0  # force non-class matches to zero IoU\n",
    "                match_counts += (ious >= iou_thresh).any(dim=1).float()\n",
    "\n",
    "        keep_mask = match_counts >= min_matches\n",
    "        kept = preds[keep_mask]\n",
    "\n",
    "        boost_mask = match_counts[keep_mask] >= window * 2\n",
    "        if boost_mask.any():\n",
    "            kept[boost_mask, 4] = torch.clamp(kept[boost_mask, 4] + boost_conf, 0.0, 1.0)\n",
    "\n",
    "        kept = remove_enclosing_boxes_vectorized(kept)\n",
    "        filtered_all.append(kept)\n",
    "    return filtered_all\n",
    "\n",
    "\n",
    "@smart_inference_mode()\n",
    "def run(\n",
    "    weights=ROOT / \"yolov5s.pt\",  # model path or triton URL\n",
    "    source=ROOT / \"data/images\",  # file/dir/URL/glob/screen/0(webcam)\n",
    "    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n",
    "    imgsz=(640, 640),  # inference size (height, width)\n",
    "    conf_thres=0.25,  # confidence threshold\n",
    "    iou_thres=0.45,  # NMS IOU threshold\n",
    "    max_det=1000,  # maximum detections per image\n",
    "    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "    view_img=False,  # show results\n",
    "    save_txt=False,  # save results to *.txt\n",
    "    save_format=0,  # save boxes coordinates in YOLO format or Pascal-VOC format (0 for YOLO and 1 for Pascal-VOC)\n",
    "    save_csv=False,  # save results in CSV format\n",
    "    save_conf=False,  # save confidences in --save-txt labels\n",
    "    save_crop=False,  # save cropped prediction boxes\n",
    "    nosave=False,  # do not save images/videos\n",
    "    classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
    "    agnostic_nms=False,  # class-agnostic NMS\n",
    "    augment=False,  # augmented inference\n",
    "    visualize=False,  # visualize features\n",
    "    update=False,  # update all models\n",
    "    project=ROOT / \"runs/detect\",  # save results to project/name\n",
    "    name=\"exp\",  # save results to project/name\n",
    "    exist_ok=False,  # existing project/name ok, do not increment\n",
    "    line_thickness=3,  # bounding box thickness (pixels)\n",
    "    hide_labels=False,  # hide labels\n",
    "    hide_conf=False,  # hide confidences\n",
    "    half=False,  # use FP16 half-precision inference\n",
    "    dnn=False,  # use OpenCV DNN for ONNX inference\n",
    "    vid_stride=1,  # video frame-rate stride\n",
    "):\n",
    "    \n",
    "    source = str(source)\n",
    "    save_img = not nosave and not source.endswith(\".txt\")  # save inference images\n",
    "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
    "    is_url = source.lower().startswith((\"rtsp://\", \"rtmp://\", \"http://\", \"https://\"))\n",
    "    webcam = source.isnumeric() or source.endswith(\".streams\") or (is_url and not is_file)\n",
    "    screenshot = source.lower().startswith(\"screen\")\n",
    "    if is_url and is_file:\n",
    "        source = check_file(source)  # download\n",
    "\n",
    "    # Directories\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "    (save_dir / \"labels\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Load model\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
    "    stride, names, pt = model.stride, model.names, model.pt\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "    # Dataloader\n",
    "    bs = 1  # batch_size\n",
    "    if webcam:\n",
    "        view_img = check_imshow(warn=True)\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "        bs = len(dataset)\n",
    "    elif screenshot:\n",
    "        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "    # Run inference\n",
    "    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
    "    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device), Profile(device=device))\n",
    "    for p in dt:\n",
    "        p.dt = 0.0\n",
    "    \n",
    "    # Storage for all predictions and metadata (memory optimized)\n",
    "    all_predictions = []\n",
    "    all_metadata = []\n",
    "    max_detections = 0  # Track max detections for tensor padding\n",
    "    \n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        # print(im0s.shape)\n",
    "        for p in dt:\n",
    "            p.dt = 0.0\n",
    "        with dt[0]:\n",
    "            im = torch.from_numpy(im).to(model.device)\n",
    "            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "            if model.xml and im.shape[0] > 1:\n",
    "                ims = torch.chunk(im, im.shape[0], 0)\n",
    "\n",
    "        # Inference\n",
    "        with dt[1]:\n",
    "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "            if model.xml and im.shape[0] > 1:\n",
    "                pred = None\n",
    "                for image in ims:\n",
    "                    if pred is None:\n",
    "                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n",
    "                    else:\n",
    "                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n",
    "                pred = [pred, None]\n",
    "            else:\n",
    "                pred = model(im, augment=augment, visualize=visualize)\n",
    "        # NMS\n",
    "        with dt[2]:\n",
    "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "\n",
    "        # Store predictions and metadata for post-processing (memory optimized)\n",
    "        # Process predictions to standardize format for tensor conversion\n",
    "        processed_pred = []\n",
    "        for p in pred:\n",
    "            if len(p) > 0:\n",
    "                processed_pred.append(p.detach().cpu())\n",
    "                max_detections = max(max_detections, len(p))\n",
    "            else:\n",
    "                # Create empty tensor with correct shape for padding\n",
    "                empty_tensor = torch.zeros((0, 6), dtype=torch.float32)  # 6 for [x1,y1,x2,y2,conf,cls]\n",
    "                processed_pred.append(empty_tensor)\n",
    "        \n",
    "        all_predictions.extend(processed_pred)  # Extend instead of append for flattening\n",
    "        \n",
    "        # Store metadata for each image in the batch\n",
    "        if webcam:  # batch_size >= 1\n",
    "            for i in range(len(pred)):\n",
    "                metadata = {\n",
    "                    'path': path[i],\n",
    "                    'im_shape': im.shape,\n",
    "                    'im0_shape': im0s[i].shape,\n",
    "                    'frame': dataset.count,\n",
    "                    's': s + f\"{i}: \",\n",
    "                    'webcam_idx': i\n",
    "                }\n",
    "                all_metadata.append(metadata)\n",
    "        else:\n",
    "            # Single image case\n",
    "            for i in range(len(pred)):\n",
    "                metadata = {\n",
    "                    'path': path,\n",
    "                    'im_shape': im.shape,\n",
    "                    'im0_shape': im0s.shape,\n",
    "                    'frame': getattr(dataset, \"frame\", 0),\n",
    "                    's': s,\n",
    "                    'webcam_idx': None\n",
    "                }\n",
    "                all_metadata.append(metadata)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del im\n",
    "        if model.xml and im.shape[0] > 1:\n",
    "            del ims\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    # Convert predictions list to tensor for temporal filtering\n",
    "    # Pad all predictions to the same size and stack into a single tensor\n",
    "    if max_detections > 0:\n",
    "        padded_predictions = []\n",
    "        for pred_tensor in all_predictions:\n",
    "            if len(pred_tensor) == 0:\n",
    "                # Create zero-padded tensor for images with no detections\n",
    "                padded_tensor = torch.zeros((max_detections, 6), dtype=torch.float32)\n",
    "            elif len(pred_tensor) < max_detections:\n",
    "                # Pad predictions to max_detections\n",
    "                padding = torch.zeros((max_detections - len(pred_tensor), 6), dtype=torch.float32)\n",
    "                padded_tensor = torch.cat([pred_tensor, padding], dim=0)\n",
    "            else:\n",
    "                padded_tensor = pred_tensor[:max_detections]  # Truncate if too many detections\n",
    "            padded_predictions.append(padded_tensor)\n",
    "        \n",
    "        # Stack all predictions into a single tensor [num_images, max_detections, 6]\n",
    "        predictions_tensor = torch.stack(padded_predictions, dim=0)\n",
    "    else:\n",
    "        # No detections in any image, create empty tensor\n",
    "        predictions_tensor = torch.zeros((len(all_predictions), 1, 6), dtype=torch.float32)\n",
    "\n",
    "    # Post-processing: Apply temporal filtering\n",
    "    with dt[3]:\n",
    "        t1 = 0\n",
    "        for pred in predictions_tensor:\n",
    "            t1 += pred.shape[0]\n",
    "        print(\"before filtering: \" + str(t1))\n",
    "        filtered_predictions_tensor = temporal_filter_preds(predictions_tensor)\n",
    "        filtered_predictions_tensor = temporal_completion(filtered_predictions_tensor)\n",
    "        t2 = 0\n",
    "        for pred in filtered_predictions_tensor:\n",
    "            t2 += pred.shape[0]\n",
    "        print(\"after filtering: \" + str(t2))\n",
    "        print(\"diff: \" + str(t1-t2))\n",
    "        \n",
    "        # filtered_predictions_tensor = predictions_tensor\n",
    "    \n",
    "    # Convert back to list format for processing\n",
    "    filtered_predictions = []\n",
    "    for i in range(len(filtered_predictions_tensor)):\n",
    "        pred_tensor = filtered_predictions_tensor[i]\n",
    "        # Remove padding (zero rows)\n",
    "        if len(pred_tensor) > 0:\n",
    "            # Keep only non-zero detections (confidence > 0)\n",
    "            valid_mask = pred_tensor[:, 4] > 0  # confidence column\n",
    "            valid_detections = pred_tensor[valid_mask]\n",
    "            filtered_predictions.append(valid_detections)\n",
    "        else:\n",
    "            filtered_predictions.append(torch.zeros((0, 6), dtype=torch.float32))\n",
    "\n",
    "    # Define the path for the CSV file\n",
    "    csv_path = save_dir / \"predictions.csv\"\n",
    "\n",
    "    # Create or append to the CSV file\n",
    "    def write_to_csv(image_name, prediction, confidence):\n",
    "        data = {\"Image Name\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n",
    "        file_exists = os.path.isfile(csv_path)\n",
    "        with open(csv_path, mode=\"a\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data.keys())\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(data)\n",
    "\n",
    "    # Process filtered predictions and write results\n",
    "    # Recreate dataset iterator for second pass\n",
    "    if webcam:\n",
    "        dataset2 = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "    elif screenshot:\n",
    "        dataset2 = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "    else:\n",
    "        dataset2 = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
    "    \n",
    "    metadata_idx = 0\n",
    "    image_idx = 0\n",
    "    for path, im, im0s, vid_cap, s in dataset2:\n",
    "        # Determine number of images in this batch\n",
    "        if webcam:\n",
    "            num_images = len(im0s)\n",
    "        else:\n",
    "            num_images = 1\n",
    "            \n",
    "        for i in range(num_images):\n",
    "            if image_idx >= len(filtered_predictions):\n",
    "                break\n",
    "                \n",
    "            det = filtered_predictions[image_idx]\n",
    "            seen += 1\n",
    "            \n",
    "            # Extract metadata\n",
    "            metadata = all_metadata[metadata_idx]\n",
    "            \n",
    "            # Get current image data\n",
    "            if webcam:  # batch_size >= 1\n",
    "                current_im0 = im0s[i].copy()\n",
    "                current_path = path[i]\n",
    "            else:\n",
    "                current_im0 = im0s.copy()\n",
    "                current_path = path\n",
    "                \n",
    "            # Move predictions back to GPU if needed\n",
    "            if len(det) > 0 and det.device != model.device:\n",
    "                det = det.to(model.device)\n",
    "            \n",
    "            frame = metadata['frame']\n",
    "            s_meta = metadata['s']\n",
    "            webcam_idx = metadata['webcam_idx']\n",
    "            \n",
    "            if webcam and webcam_idx is not None:\n",
    "                i = webcam_idx\n",
    "            else:\n",
    "                i = 0\n",
    "\n",
    "            p = Path(current_path)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # im.jpg\n",
    "            txt_path = str(save_dir / \"labels\" / p.stem) + (\"\" if dataset.mode == \"image\" else f\"_{frame}\")  # im.txt\n",
    "            # print(\"im.shape: \" + str(im.shape))\n",
    "            s_meta += \"{:g}x{:g} \".format(*im.shape[1:])  # print string\n",
    "            \n",
    "            gn = torch.tensor(current_im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            # print(\"gain: \" + str(gn))\n",
    "            imc = current_im0.copy() if save_crop else current_im0  # for save_crop\n",
    "            annotator = Annotator(current_im0, line_width=line_thickness, example=str(names))\n",
    "            \n",
    "            metadata_idx += 1\n",
    "            image_idx += 1\n",
    "            \n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_boxes(im.shape[1:], det[:, :4], current_im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, 5].unique():\n",
    "                    n = (det[:, 5] == c).sum()  # detections per class\n",
    "                    s_meta += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    c = int(cls)  # integer class\n",
    "                    label = names[c] if hide_conf else f\"{names[c]}\"\n",
    "                    confidence = float(conf)\n",
    "                    confidence_str = f\"{confidence:.2f}\"\n",
    "                    total_time_ms = sum(p.dt for p in dt) * 1000\n",
    "\n",
    "                    if save_csv:\n",
    "                        write_to_csv(p.name, label, confidence_str)\n",
    "\n",
    "                    if save_txt:  # Write to file\n",
    "                        if save_format == 0:\n",
    "                            coords = (\n",
    "                                (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n",
    "                            )  # normalized xywh\n",
    "                        else:\n",
    "                            coords = (torch.tensor(xyxy).view(1, 4) / gn).view(-1).tolist()  # xyxy\n",
    "                        # print(coords)\n",
    "                        line = (cls, *coords, conf, total_time_ms) if save_conf else (cls, *coords, total_time_ms)  # label format\n",
    "                        # print(\"Line: \" + str(line))\n",
    "                        with open(f\"{txt_path}.txt\", \"a\") as f:\n",
    "                            f.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n",
    "\n",
    "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                        c = int(cls)  # integer class\n",
    "                        label = None if hide_labels else (names[c] if hide_conf else f\"{names[c]} {conf:.2f}\")\n",
    "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                    if save_crop:\n",
    "                        save_one_box(xyxy, imc, file=save_dir / \"crops\" / names[c] / f\"{p.stem}.jpg\", BGR=True)\n",
    "\n",
    "            # Stream results\n",
    "            current_im0 = annotator.result()\n",
    "            if view_img:\n",
    "                if platform.system() == \"Linux\" and p not in windows:\n",
    "                    windows.append(p)\n",
    "                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
    "                    cv2.resizeWindow(str(p), current_im0.shape[1], current_im0.shape[0])\n",
    "                cv2.imshow(str(p), current_im0)\n",
    "                cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == \"image\":\n",
    "                    cv2.imwrite(save_path, current_im0)\n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path[i] != save_path:  # new video\n",
    "                        vid_path[i] = save_path\n",
    "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                            vid_writer[i].release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, current_im0.shape[1], current_im0.shape[0]\n",
    "                        save_path = str(Path(save_path).with_suffix(\".mp4\"))  # force *.mp4 suffix on results videos\n",
    "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "                    vid_writer[i].write(current_im0)\n",
    "\n",
    "            # Print time (inference-only)\n",
    "            # LOGGER.info(f\"{s_meta}{'' if len(det) else '(no detections), '}{dt[1].dt * 1e3:.1f}ms\")\n",
    "\n",
    "    # Print results\n",
    "    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n",
    "    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS, %.1fms post-process per image at shape {(1, 3, *imgsz)}\" % t)\n",
    "    if save_txt or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else \"\"\n",
    "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "    if update:\n",
    "        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s.pt\", help=\"model path or triton URL\")\n",
    "    parser.add_argument(\"--source\", type=str, default=ROOT / \"data/images\", help=\"file/dir/URL/glob/screen/0(webcam)\")\n",
    "    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"(optional) dataset.yaml path\")\n",
    "    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", nargs=\"+\", type=int, default=[640], help=\"inference size h,w\")\n",
    "    parser.add_argument(\"--conf-thres\", type=float, default=0.25, help=\"confidence threshold\")\n",
    "    parser.add_argument(\"--iou-thres\", type=float, default=0.45, help=\"NMS IoU threshold\")\n",
    "    parser.add_argument(\"--max-det\", type=int, default=1000, help=\"maximum detections per image\")\n",
    "    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n",
    "    parser.add_argument(\"--view-img\", action=\"store_true\", help=\"show results\")\n",
    "    parser.add_argument(\"--save-txt\", action=\"store_true\", help=\"save results to *.txt\")\n",
    "    parser.add_argument(\n",
    "        \"--save-format\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"whether to save boxes coordinates in YOLO format or Pascal-VOC format when save-txt is True, 0 for YOLO and 1 for Pascal-VOC\",\n",
    "    )\n",
    "    parser.add_argument(\"--save-csv\", action=\"store_true\", help=\"save results in CSV format\")\n",
    "    parser.add_argument(\"--save-conf\", action=\"store_true\", help=\"save confidences in --save-txt labels\")\n",
    "    parser.add_argument(\"--save-crop\", action=\"store_true\", help=\"save cropped prediction boxes\")\n",
    "    parser.add_argument(\"--nosave\", action=\"store_true\", help=\"do not save images/videos\")\n",
    "    parser.add_argument(\"--classes\", nargs=\"+\", type=int, help=\"filter by class: --classes 0, or --classes 0 2 3\")\n",
    "    parser.add_argument(\"--agnostic-nms\", action=\"store_true\", help=\"class-agnostic NMS\")\n",
    "    parser.add_argument(\"--augment\", action=\"store_true\", help=\"augmented inference\")\n",
    "    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"visualize features\")\n",
    "    parser.add_argument(\"--update\", action=\"store_true\", help=\"update all models\")\n",
    "    parser.add_argument(\"--project\", default=ROOT / \"runs/detect\", help=\"save results to project/name\")\n",
    "    parser.add_argument(\"--name\", default=\"exp\", help=\"save results to project/name\")\n",
    "    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n",
    "    parser.add_argument(\"--line-thickness\", default=3, type=int, help=\"bounding box thickness (pixels)\")\n",
    "    parser.add_argument(\"--hide-labels\", default=False, action=\"store_true\", help=\"hide labels\")\n",
    "    parser.add_argument(\"--hide-conf\", default=False, action=\"store_true\", help=\"hide confidences\")\n",
    "    parser.add_argument(\"--half\", action=\"store_true\", help=\"use FP16 half-precision inference\")\n",
    "    parser.add_argument(\"--dnn\", action=\"store_true\", help=\"use OpenCV DNN for ONNX inference\")\n",
    "    parser.add_argument(\"--vid-stride\", type=int, default=1, help=\"video frame-rate stride\")\n",
    "    opt = parser.parse_args()\n",
    "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
    "    print_args(vars(opt))\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    \n",
    "    check_requirements(ROOT / \"requirements.txt\", exclude=(\"tensorboard\", \"thop\"))\n",
    "    run(**vars(opt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"detect2.py\", 'w') as f:\n",
    "    f.write(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:26:24.975016Z",
     "iopub.status.busy": "2025-07-12T16:26:24.974749Z",
     "iopub.status.idle": "2025-07-12T16:29:32.492051Z",
     "shell.execute_reply": "2025-07-12T16:29:32.491118Z",
     "shell.execute_reply.started": "2025-07-12T16:26:24.974997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[34m\u001b[1mdetect2: \u001b[0mweights=['/kaggle/input/yolov5m_fused/pytorch/default/1/best_fused.pt'], source=/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=0,1, view_img=False, save_txt=True, save_format=1, save_csv=True, save_conf=True, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ğŸš€ v7.0-422-g2540fd4c Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "                                                              CUDA:1 (Tesla T4, 15095MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
      "before filtering: 253500\n",
      "after filtering: 8304\n",
      "diff: 245196\n",
      "Speed: 0.4ms pre-process, 6.1ms inference, 0.9ms NMS, 1.0ms post-process per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n",
      "5787 labels saved to runs/detect/exp/labels\n"
     ]
    }
   ],
   "source": [
    "!python detect2.py --weights \"$weights_path\" --source \"$images_path_RGB\" --conf-thres 0.1 --save-format 1 --device \"$device\" --save-txt --save-csv --save-conf --nosave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:31:57.187875Z",
     "iopub.status.busy": "2025-07-12T16:31:57.187283Z",
     "iopub.status.idle": "2025-07-12T16:31:57.198928Z",
     "shell.execute_reply": "2025-07-12T16:31:57.198212Z",
     "shell.execute_reply.started": "2025-07-12T16:31:57.187836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "obj = r\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils.general import set_logging\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, Profile, check_file, check_img_size, \n",
    "                            check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args,\n",
    "                            scale_boxes, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0] \n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT)) \n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd())) \n",
    "\n",
    "#---------------Object Tracking---------------\n",
    "import skimage\n",
    "import supervision as sv\n",
    "\n",
    "#-----------Object Blurring-------------------\n",
    "blurratio = 40\n",
    "\n",
    "\n",
    "#.................. Tracker Functions .................\n",
    "'''Computer Color for every box and track'''\n",
    "palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "def compute_color_for_labels(label):\n",
    "    color = [int(int(p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "    return tuple(color)\n",
    "\n",
    "\n",
    "def bbox_rel(*xyxy):\n",
    "    bbox_left = min([xyxy[0].item(), xyxy[2].item()])\n",
    "    bbox_top = min([xyxy[1].item(), xyxy[3].item()])\n",
    "    bbox_w = abs(xyxy[0].item() - xyxy[2].item())\n",
    "    bbox_h = abs(xyxy[1].item() - xyxy[3].item())\n",
    "    x_c = (bbox_left + bbox_w / 2)\n",
    "    y_c = (bbox_top + bbox_h / 2)\n",
    "    w = bbox_w\n",
    "    h = bbox_h\n",
    "    return x_c, y_c, w, h\n",
    "\n",
    "\n",
    "def draw_boxes(img, bbox, identities=None, categories=None, \n",
    "                names=None, color_box=None,offset=(0, 0)):\n",
    "    for i, box in enumerate(bbox):\n",
    "        x1, y1, x2, y2 = [int(i) for i in box]\n",
    "        x1 += offset[0]\n",
    "        x2 += offset[0]\n",
    "        y1 += offset[1]\n",
    "        y2 += offset[1]\n",
    "        cat = int(categories[i]) if categories is not None else 0\n",
    "        id = int(identities[i]) if identities is not None else 0\n",
    "        data = (int((box[0]+box[2])/2),(int((box[1]+box[3])/2)))\n",
    "        label = str(id)\n",
    "\n",
    "        if color_box:\n",
    "            color = compute_color_for_labels(id)\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2),color, 2)\n",
    "            cv2.rectangle(img, (x1, y1 - 20), (x1 + w, y1), (255,191,0), -1)\n",
    "            cv2.putText(img, label, (x1, y1 - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "            [255, 255, 255], 1)\n",
    "            cv2.circle(img, data, 3, color,-1)\n",
    "        else:\n",
    "            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2),(255,191,0), 2)\n",
    "            cv2.rectangle(img, (x1, y1 - 20), (x1 + w, y1), (255,191,0), -1)\n",
    "            cv2.putText(img, label, (x1, y1 - 5),cv2.FONT_HERSHEY_SIMPLEX, 0.6, \n",
    "            [255, 255, 255], 1)\n",
    "            cv2.circle(img, data, 3, (255,191,0),-1)\n",
    "    return img\n",
    "#..............................................................................\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def detect(weights=ROOT / 'yolov5n.pt',\n",
    "        source=ROOT / 'yolov5/data/images', \n",
    "        data=ROOT / 'yolov5/data/coco128.yaml',  \n",
    "        imgsz=(640, 640),conf_thres=0.25,iou_thres=0.45,  \n",
    "        max_det=1000, device='cpu',  view_img=False,  \n",
    "        save_txt=False, save_conf=False, save_crop=False, \n",
    "        nosave=False, classes=None,  agnostic_nms=False,  \n",
    "        augment=False, visualize=False,  update=False,  \n",
    "        project=ROOT / 'runs/detect',  name='exp',  \n",
    "        exist_ok=False, line_thickness=2,hide_labels=False,  \n",
    "        hide_conf=False,half=False,dnn=False,display_labels=False,\n",
    "        blur_obj=False,color_box = False,):\n",
    "    \n",
    "    save_img = not nosave and not source.endswith('.txt') \n",
    "    \n",
    "    #.... Initialize SORT .... \n",
    "    \n",
    "    track_color_id = 0\n",
    "\n",
    "    byte_tracker = sv.ByteTrack()\n",
    "    smoother = sv.DetectionsSmoother()\n",
    "    label = sv.LabelAnnotator()\n",
    "    trace_annotator = sv.TraceAnnotator()\n",
    "    video_info = sv.VideoInfo.from_video_path(source)\n",
    "    frame_generator = sv.get_video_frames_generator(source)\n",
    "    fps_monitor = sv.FPSMonitor()\n",
    "    #......................... \n",
    "    \n",
    "    \n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  \n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "    set_logging()\n",
    "    device = select_device(device)\n",
    "    half &= device.type != 'cpu'  \n",
    "\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n",
    "    stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
    "    imgsz = check_img_size(imgsz, s=stride)  \n",
    "\n",
    "    half &= (pt or jit or onnx or engine) and device.type != 'cpu'  \n",
    "    if pt or jit:\n",
    "        model.model.half() if half else model.model.float()\n",
    "\n",
    "    if webcam:\n",
    "        cudnn.benchmark = True  \n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = len(dataset) \n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = 1 \n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "    \n",
    "    with sv.VideoSink(target_path=\"output.mp4\", video_info=video_info) as sink:\n",
    "        for path, im, im0s, vid_cap, s in dataset:\n",
    "\n",
    "            svframe = next(frame_generator)\n",
    "            t1 = time_sync()\n",
    "            im = torch.from_numpy(im).to(device)\n",
    "            im = im.half() if half else im.float()  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "            t2 = time_sync()\n",
    "            dt[0] += t2 - t1\n",
    "    \n",
    "            # Inference\n",
    "            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "            pred = model(im, augment=augment, visualize=visualize)\n",
    "            t3 = time_sync()\n",
    "            dt[1] += t3 - t2\n",
    "    \n",
    "            # NMS\n",
    "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "            dt[2] += time_sync() - t3\n",
    "    \n",
    "            \n",
    "            for i, det in enumerate(pred):  # per image\n",
    "                seen += 1\n",
    "                if webcam:  # batch_size >= 1\n",
    "                    p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "                    s += f'{i}: '\n",
    "                else:\n",
    "                    p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "    \n",
    "                p = Path(p)\n",
    "                save_path = str(save_dir / p.name)\n",
    "                txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "                s += '%gx%g ' % im.shape[2:]\n",
    "                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
    "                # print(gn)\n",
    "                imc = im0.copy() if save_crop else im0\n",
    "                annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "                if len(det):\n",
    "                    det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "                    for c in det[:, -1].unique():\n",
    "                        n = (det[:, -1] == c).sum()\n",
    "                        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"\n",
    "    \n",
    "                    # Write results\n",
    "                    for *xyxy, conf, cls in reversed(det):\n",
    "                        if blur_obj:\n",
    "                            crop_obj = im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])]\n",
    "                            blur = cv2.blur(crop_obj,(blurratio,blurratio))\n",
    "                            im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2])] = blur\n",
    "                        else:\n",
    "                            continue\n",
    "                    #..................USE TRACK FUNCTION....................\n",
    "                    \n",
    "                    \n",
    "                    # Run bytetrack\n",
    "                    pred_np = det.cpu().numpy()\n",
    "                    xyxy = pred_np[:, :4]\n",
    "                    confidence = pred_np[:, 4]\n",
    "                    class_id = pred_np[:, 5].astype(int)\n",
    "                    detections = sv.Detections(\n",
    "                        xyxy=xyxy,\n",
    "                        confidence=confidence,\n",
    "                        class_id=class_id\n",
    "                    )\n",
    "                    detections = byte_tracker.update_with_detections(detections)\n",
    "                    detections = smoother.update_with_detections(detections)\n",
    "                    # print(\"Track_id: \" + str(detections.tracker_id) + \", track_confidence: \" + str(detections.confidence))\n",
    "                    labels = [ f\"{tracker_id}, {confidence:.2f}\" for tracker_id, confidence in zip(detections.tracker_id, detections.confidence) ]\n",
    "                    annotated_frame = label.annotate(scene=svframe.copy(), detections=detections, labels=labels)\n",
    "                    annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "                    sink.write_frame(frame=annotated_frame)\n",
    "                    \n",
    "                        \n",
    "                    if save_txt:\n",
    "                        os.makedirs(save_dir / \"labels\", exist_ok=True)\n",
    "                        frame_txt_path = f\"{save_dir / 'labels' / p.stem}_{frame}.txt\"\n",
    "                        with open(frame_txt_path, 'w') as f:\n",
    "                            for box, tracker_id, class_id, track_conf, det_conf in zip(\n",
    "                                    detections.xyxy, detections.tracker_id, detections.class_id, detections.confidence, confidence):\n",
    "                                coords = (torch.tensor(box).view(1, 4) / gn).view(-1).tolist()  # xyxy\n",
    "                                x1, y1, x2, y2 = coords\n",
    "                                line = f\"{int(tracker_id)} {int(class_id)} {track_conf:.2f} {det_conf:.2f} {x1:.4f} {y1:.4f} {x2:.4f} {y2:.4f}\\n\"\n",
    "                                f.write(line)\n",
    "\n",
    "\n",
    "    \n",
    "                if view_img:\n",
    "                    cv2.imshow(str(p), im0)\n",
    "                    cv2.waitKey(1) \n",
    "                if save_img:\n",
    "                    if dataset.mode == 'image':\n",
    "                        cv2.imwrite(save_path, im0)\n",
    "                    else:\n",
    "                        if vid_path != save_path: \n",
    "                            vid_path = save_path\n",
    "                            if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                                vid_writer.release()  \n",
    "                            if vid_cap: \n",
    "                                fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                                w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                                h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                            else:\n",
    "                                fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                                save_path += '.mp4'\n",
    "                            vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                        vid_writer.write(im0)\n",
    "            #print(\"Frame Processing!\")\n",
    "            fps_monitor.tick()\n",
    "    print(\"Video Exported Success\")\n",
    "    print(str(fps_monitor.fps))\n",
    "\n",
    "    if update:\n",
    "        strip_optimizer(weights)\n",
    "    \n",
    "    if vid_cap:\n",
    "        vid_cap.release()\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
    "    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')\n",
    "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
    "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--view-img', action='store_true', help='show results')\n",
    "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
    "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
    "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n",
    "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "    parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
    "    parser.add_argument('--update', action='store_true', help='update all models')\n",
    "    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n",
    "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
    "    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
    "    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
    "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
    "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
    "    parser.add_argument('--blur-obj', action='store_true', help='Blur Detected Objects')\n",
    "    parser.add_argument('--color-box', action='store_true', help='Change color of every box and track')\n",
    "    opt = parser.parse_args()\n",
    "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
    "    print_args(vars(opt))\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    check_requirements(exclude=('tensorboard', 'thop'))\n",
    "    detect(**vars(opt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n",
    "\"\"\"\n",
    "\n",
    "with open('obj_det_and_trk2.py', 'w') as f:\n",
    "    f.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:32:06.780845Z",
     "iopub.status.busy": "2025-07-12T16:32:06.780614Z",
     "iopub.status.idle": "2025-07-12T16:37:24.160963Z",
     "shell.execute_reply": "2025-07-12T16:37:24.160301Z",
     "shell.execute_reply.started": "2025-07-12T16:32:06.780829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: V_DRONE_099\n",
      "Processing video: V_BIRD_043\n",
      "Processing video: V_BIRD_050\n",
      "Processing video: V_BIRD_051\n",
      "Processing video: V_BIRD_045\n",
      "Processing video: V_DRONE_111\n",
      "Processing video: V_DRONE_104\n",
      "Processing video: V_DRONE_101\n",
      "Processing video: V_BIRD_047\n",
      "Processing video: V_DRONE_114\n",
      "Processing video: V_DRONE_105\n",
      "Processing video: V_DRONE_108\n",
      "{'V_DRONE_099': {'0,1': 73.77173546171838, 'cpu': 35.75966579445679}, 'V_BIRD_043': {'0,1': 72.93690789954962, 'cpu': 33.734838673746076}, 'V_BIRD_050': {'0,1': 77.63968042327646, 'cpu': 33.90851315642103}, 'V_BIRD_051': {'0,1': 77.57402924520275, 'cpu': 36.23406015834235}, 'V_BIRD_045': {'0,1': 75.5086905330378, 'cpu': 35.304686665735005}, 'V_DRONE_111': {'0,1': 76.66514451432114, 'cpu': 35.65488730370775}, 'V_DRONE_104': {'0,1': 71.59304007402795, 'cpu': 35.89448891081313}, 'V_DRONE_101': {'0,1': 74.0105357550465, 'cpu': 34.965468080412656}, 'V_BIRD_047': {'0,1': 70.94680747031389, 'cpu': 34.26383978049131}, 'V_DRONE_114': {'0,1': 69.6933934403322, 'cpu': 35.78893704855911}, 'V_DRONE_105': {'0,1': 78.50625068207023, 'cpu': 35.18419832055039}, 'V_DRONE_108': {'0,1': 72.75395376270497, 'cpu': 33.86906517451641}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "conf_thres = 0.01\n",
    "\n",
    "fps_results = {}\n",
    "\n",
    "videos = glob.glob(os.path.join(videos_path_RGB, \"*.mp4\"))\n",
    "\n",
    "for video in videos:\n",
    "    video_name = os.path.splitext(os.path.basename(video))[0]\n",
    "    fps_results[video_name] = {}\n",
    "    print(\"Processing video: \" + video_name)\n",
    "\n",
    "    for device in devices:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"obj_det_and_trk2.py\", \"--weights\", weights_path, \"--source\",  video, \"--imgsz\", \"320\", \"--conf-thres\", str(conf_thres), \"--save-txt\", \"--save-conf\", \"--project\", project_path, \"--device\", device],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            fps_value = float(result.stdout.strip().split('\\n')[-1])\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting FPS for {video} on {device}: {e}\")\n",
    "            fps_value = -1\n",
    "\n",
    "        fps_results[video_name][device] = fps_value\n",
    "\n",
    "# Optional: Print or save the results\n",
    "print(fps_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !python obj_det_and_trk2.py --weights /kaggle/input/yolov5m/pytorch/default/1/best.pt --source /kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/RGB/videos/V_DRONE_101.mp4 --imgsz 320 --conf-thres 0.01 --save-txt --save-conf --project=/kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:38:04.051900Z",
     "iopub.status.busy": "2025-07-12T16:38:04.051203Z",
     "iopub.status.idle": "2025-07-12T16:38:04.804843Z",
     "shell.execute_reply": "2025-07-12T16:38:04.804118Z",
     "shell.execute_reply.started": "2025-07-12T16:38:04.051857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 exp directories.\n",
      "Processing label directory: /kaggle/working/exp/labels\n",
      "Updated all label files in /kaggle/working/exp/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp2/labels\n",
      "Updated all label files in /kaggle/working/exp2/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp3/labels\n",
      "Updated all label files in /kaggle/working/exp3/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp4/labels\n",
      "Updated all label files in /kaggle/working/exp4/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp5/labels\n",
      "Updated all label files in /kaggle/working/exp5/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp6/labels\n",
      "Updated all label files in /kaggle/working/exp6/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp7/labels\n",
      "Updated all label files in /kaggle/working/exp7/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp8/labels\n",
      "Updated all label files in /kaggle/working/exp8/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp9/labels\n",
      "Updated all label files in /kaggle/working/exp9/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp10/labels\n",
      "Updated all label files in /kaggle/working/exp10/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp11/labels\n",
      "Updated all label files in /kaggle/working/exp11/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp12/labels\n",
      "Updated all label files in /kaggle/working/exp12/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp13/labels\n",
      "Updated all label files in /kaggle/working/exp13/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp14/labels\n",
      "Updated all label files in /kaggle/working/exp14/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp15/labels\n",
      "Updated all label files in /kaggle/working/exp15/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp16/labels\n",
      "Updated all label files in /kaggle/working/exp16/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp17/labels\n",
      "Updated all label files in /kaggle/working/exp17/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp18/labels\n",
      "Updated all label files in /kaggle/working/exp18/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp19/labels\n",
      "Updated all label files in /kaggle/working/exp19/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp20/labels\n",
      "Updated all label files in /kaggle/working/exp20/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp21/labels\n",
      "Updated all label files in /kaggle/working/exp21/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp22/labels\n",
      "Updated all label files in /kaggle/working/exp22/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp23/labels\n",
      "Updated all label files in /kaggle/working/exp23/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp24/labels\n",
      "Updated all label files in /kaggle/working/exp24/labels with trend column.\n",
      "All exp directories processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = \"/kaggle/working\"\n",
    "exp_dirs = sorted(glob.glob(os.path.join(base_dir, \"exp*\")), key=os.path.getmtime)\n",
    "\n",
    "print(f\"Found {len(exp_dirs)} exp directories.\")\n",
    "\n",
    "for exp_dir in exp_dirs:\n",
    "    labels_dir = os.path.join(exp_dir, \"labels\")\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        print(f\"Skipping {exp_dir}, no 'labels' folder found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing label directory: {labels_dir}\")\n",
    "\n",
    "    # Collect bounding box area per track_id per frame\n",
    "    track_areas = defaultdict(list)  # track_id: list of (frame_number, area)\n",
    "    file_list = sorted(glob.glob(os.path.join(labels_dir, \"*.txt\")))\n",
    "    frame_index_map = {file: idx for idx, file in enumerate(file_list)}\n",
    "\n",
    "    for txt_file in file_list:\n",
    "        frame_id = frame_index_map[txt_file]\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "                track_id = int(parts[0])\n",
    "                x1, y1, x2, y2 = map(float, parts[4:8])\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                track_areas[track_id].append((frame_id, area))\n",
    "\n",
    "    # Compute trend per track_id\n",
    "    track_trend = {}\n",
    "    for track_id, entries in track_areas.items():\n",
    "        entries.sort()\n",
    "        frame_ids, areas = zip(*entries)\n",
    "        if len(areas) < 3:\n",
    "            trend = \"stationary\"\n",
    "        else:\n",
    "            slope = np.polyfit(frame_ids, areas, 1)[0]\n",
    "            if slope > 0:\n",
    "                trend = \"approaching\"\n",
    "            elif slope < -0:\n",
    "                trend = \"receding\"\n",
    "            else:\n",
    "                trend = \"stationary\"\n",
    "        track_trend[track_id] = trend\n",
    "\n",
    "    # Rewrite each txt file with trend info as final column\n",
    "    for txt_file in file_list:\n",
    "        new_lines = []\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 8:\n",
    "                    new_lines.append(line)\n",
    "                    continue\n",
    "                track_id = int(parts[0])\n",
    "                trend = track_trend.get(track_id, \"stationary\")\n",
    "                new_line = line.strip() + f\" {trend}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        with open(txt_file, \"w\") as f:\n",
    "            f.writelines(new_lines)\n",
    "\n",
    "    print(f\"Updated all label files in {labels_dir} with trend column.\")\n",
    "\n",
    "print(\"All exp directories processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:38:12.377708Z",
     "iopub.status.busy": "2025-07-12T16:38:12.377433Z",
     "iopub.status.idle": "2025-07-12T16:38:12.383040Z",
     "shell.execute_reply": "2025-07-12T16:38:12.382329Z",
     "shell.execute_reply.started": "2025-07-12T16:38:12.377687Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:39:39.079423Z",
     "iopub.status.busy": "2025-07-12T16:39:39.078858Z",
     "iopub.status.idle": "2025-07-12T16:39:39.404938Z",
     "shell.execute_reply": "2025-07-12T16:39:39.404357Z",
     "shell.execute_reply.started": "2025-07-12T16:39:39.079396Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV saved to RGB.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "submission_rows = []\n",
    "\n",
    "# ---------- PART 1: Image Detection Labels ----------\n",
    "detection_dir = os.path.join(\"yolov5\", \"runs\", \"detect\", \"exp\", \"labels\")\n",
    "for txt_file in sorted(glob.glob(os.path.join(detection_dir, \"*.txt\"))):\n",
    "    frame_name = os.path.basename(txt_file).replace(\".txt\", \"\")\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 7:\n",
    "                continue\n",
    "            class_id, conf_det, x_min, y_min, x_max, y_max, infer_time = map(float, parts)\n",
    "            class_label = \"bird\" if int(class_id) == 0 else \"drone\"\n",
    "            row = [\n",
    "                frame_name,\n",
    "                0,\n",
    "                x_min, y_min, x_max, y_max,\n",
    "                class_label,\n",
    "                \"0\",\n",
    "                round(conf_det, 2),\n",
    "                round(infer_time, 2),\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0\n",
    "            ]\n",
    "            submission_rows.append(row)\n",
    "\n",
    "# ---------- PART 2: Video Tracking Labels ----------\n",
    "tracking_base_dir = \".\"\n",
    "exp_dirs = sorted([d for d in os.listdir(tracking_base_dir) if d.startswith(\"exp\") and os.path.isdir(os.path.join(tracking_base_dir, d))])\n",
    "\n",
    "for i, exp in enumerate(exp_dirs):\n",
    "    labels_dir = os.path.join(tracking_base_dir, exp, \"labels\")\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        continue\n",
    "\n",
    "    # Determine device by exp index (odd = GPU, even = CPU)\n",
    "    if i % 2 == 0:\n",
    "        continue\n",
    "\n",
    "    for txt_file in sorted(glob.glob(os.path.join(labels_dir, \"*.txt\"))):\n",
    "        frame_name = os.path.basename(txt_file).replace(\".txt\", \"\")\n",
    "        video_name = \"_\".join(frame_name.split(\"_\")[:3])\n",
    "\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                track_id, class_id, conf_track, conf_det, x_min, y_min, x_max, y_max, direction = parts\n",
    "                class_label = \"bird\" if int(class_id) == 0 else \"drone\"\n",
    "                row = [\n",
    "                    frame_name,\n",
    "                    int(track_id),\n",
    "                    float(x_min), float(y_min), float(x_max), float(y_max),\n",
    "                    class_label,\n",
    "                    direction,\n",
    "                    round(float(conf_det), 2),\n",
    "                    0,\n",
    "                    round(fps_results.get(video_name, {}).get(\"cpu\", 0), 2),\n",
    "                    round(fps_results.get(video_name, {}).get(\"0,1\", 0), 2),\n",
    "                    round(float(conf_track), 2),\n",
    "                    0,\n",
    "                    0\n",
    "                ]\n",
    "                submission_rows.append(row)\n",
    "\n",
    "# ---------- PART 3: Write Final CSV ----------\n",
    "csv_path = \"RGB.csv\"\n",
    "header = [\n",
    "    \"Frame_name\", \"track_id\", \"x_min_norm\", \"y_min_norm\", \"x_max_norm\", \"y_max_norm\",\n",
    "    \"class_label\", \"direction\", \"confidence_detection\", \"inference_time_detection (ms)\",\n",
    "    \"FPS (CPU)\", \"FPS (GPU)\", \"confidence_track\", \"payload_label\", \"prob_harmful\"\n",
    "]\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(submission_rows)\n",
    "\n",
    "print(f\"Submission CSV saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:40:19.430371Z",
     "iopub.status.busy": "2025-07-12T16:40:19.429720Z",
     "iopub.status.idle": "2025-07-12T16:40:19.681653Z",
     "shell.execute_reply": "2025-07-12T16:40:19.680903Z",
     "shell.execute_reply.started": "2025-07-12T16:40:19.430344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf exp*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:40:29.727525Z",
     "iopub.status.busy": "2025-07-12T16:40:29.727267Z",
     "iopub.status.idle": "2025-07-12T16:40:29.733011Z",
     "shell.execute_reply": "2025-07-12T16:40:29.732456Z",
     "shell.execute_reply.started": "2025-07-12T16:40:29.727504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:40:42.212540Z",
     "iopub.status.busy": "2025-07-12T16:40:42.211990Z",
     "iopub.status.idle": "2025-07-12T16:43:40.446652Z",
     "shell.execute_reply": "2025-07-12T16:43:40.445975Z",
     "shell.execute_reply.started": "2025-07-12T16:40:42.212518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect2: \u001b[0mweights=['/kaggle/input/yolov5m_fused/pytorch/default/1/best_fused.pt'], source=/kaggle/input/test-detection-and-tracking/Test_detection_and_tracking/IR/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=0,1, view_img=False, save_txt=True, save_format=1, save_csv=True, save_conf=True, save_crop=False, nosave=True, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ğŸš€ v7.0-422-g2540fd4c Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
      "                                                              CUDA:1 (Tesla T4, 15095MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
      "before filtering: 71500\n",
      "after filtering: 8267\n",
      "diff: 63233\n",
      "Speed: 0.4ms pre-process, 6.1ms inference, 0.8ms NMS, 0.8ms post-process per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n",
      "5149 labels saved to runs/detect/exp2/labels\n"
     ]
    }
   ],
   "source": [
    "!python detect2.py --weights \"$weights_path\" --source \"$images_path_IR\" --conf-thres 0.1 --save-format 1 --device \"$device\" --save-txt --save-csv --save-conf --nosave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:44:27.721344Z",
     "iopub.status.busy": "2025-07-12T16:44:27.720770Z",
     "iopub.status.idle": "2025-07-12T16:49:48.956977Z",
     "shell.execute_reply": "2025-07-12T16:49:48.956197Z",
     "shell.execute_reply.started": "2025-07-12T16:44:27.721313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: IR_DRONE_155\n",
      "Processing video: IR_DRONE_139\n",
      "Processing video: IR_DRONE_142\n",
      "Processing video: IR_BIRD_079\n",
      "Processing video: IR_DRONE_157\n",
      "Processing video: IR_BIRD_078\n",
      "Processing video: IR_BIRD_075\n",
      "Processing video: IR_BIRD_077\n",
      "Processing video: IR_DRONE_152\n",
      "Processing video: IR_DRONE_147\n",
      "Processing video: IR_BIRD_076\n",
      "Processing video: IR_DRONE_145\n",
      "Processing video: IR_BIRD_074\n",
      "{'IR_DRONE_155': {'0,1': 91.68089569444193, 'cpu': 37.53100318889161}, 'IR_DRONE_139': {'0,1': 87.86592103298871, 'cpu': 38.764205100968496}, 'IR_DRONE_142': {'0,1': 96.6263782553091, 'cpu': 38.844087535417586}, 'IR_BIRD_079': {'0,1': 83.64343346322218, 'cpu': 36.55507930176884}, 'IR_DRONE_157': {'0,1': 100.38484303038517, 'cpu': 38.57414955887869}, 'IR_BIRD_078': {'0,1': 92.80070160297196, 'cpu': 34.844410195951966}, 'IR_BIRD_075': {'0,1': 93.72565154017948, 'cpu': 40.92524006462519}, 'IR_BIRD_077': {'0,1': 91.25720368014298, 'cpu': 39.856251096870636}, 'IR_DRONE_152': {'0,1': 92.4109662541655, 'cpu': 27.11507438817143}, 'IR_DRONE_147': {'0,1': 87.58317785346675, 'cpu': 36.52145762303137}, 'IR_BIRD_076': {'0,1': 93.78996361043039, 'cpu': 37.896695263532}, 'IR_DRONE_145': {'0,1': 80.96540034299062, 'cpu': 34.22449300414837}, 'IR_BIRD_074': {'0,1': 138.51502337416136, 'cpu': 42.3721248818319}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "conf_thres = 0.01\n",
    "\n",
    "fps_results = {}\n",
    "\n",
    "videos = glob.glob(os.path.join(videos_path_IR, \"*.mp4\"))\n",
    "\n",
    "for video in videos:\n",
    "    video_name = os.path.splitext(os.path.basename(video))[0]\n",
    "    fps_results[video_name] = {}\n",
    "    print(\"Processing video: \" + video_name)\n",
    "\n",
    "    for device in devices:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"obj_det_and_trk2.py\", \"--weights\", weights_path, \"--source\",  video, \"--imgsz\", \"320\", \"--conf-thres\", str(conf_thres), \"--save-txt\", \"--save-conf\", \"--project\", project_path, \"--device\", device],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            fps_value = float(result.stdout.strip().split('\\n')[-1])\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting FPS for {video} on {device}: {e}\")\n",
    "            fps_value = -1\n",
    "\n",
    "        fps_results[video_name][device] = fps_value\n",
    "\n",
    "# Optional: Print or save the results\n",
    "print(fps_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:51:47.386384Z",
     "iopub.status.busy": "2025-07-12T16:51:47.386040Z",
     "iopub.status.idle": "2025-07-12T16:51:48.218527Z",
     "shell.execute_reply": "2025-07-12T16:51:48.217964Z",
     "shell.execute_reply.started": "2025-07-12T16:51:47.386359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 exp directories.\n",
      "Processing label directory: /kaggle/working/exp/labels\n",
      "Updated all label files in /kaggle/working/exp/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp2/labels\n",
      "Updated all label files in /kaggle/working/exp2/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp3/labels\n",
      "Updated all label files in /kaggle/working/exp3/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp4/labels\n",
      "Updated all label files in /kaggle/working/exp4/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp5/labels\n",
      "Updated all label files in /kaggle/working/exp5/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp6/labels\n",
      "Updated all label files in /kaggle/working/exp6/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp7/labels\n",
      "Updated all label files in /kaggle/working/exp7/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp8/labels\n",
      "Updated all label files in /kaggle/working/exp8/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp9/labels\n",
      "Updated all label files in /kaggle/working/exp9/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp10/labels\n",
      "Updated all label files in /kaggle/working/exp10/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp11/labels\n",
      "Updated all label files in /kaggle/working/exp11/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp12/labels\n",
      "Updated all label files in /kaggle/working/exp12/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp13/labels\n",
      "Updated all label files in /kaggle/working/exp13/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp14/labels\n",
      "Updated all label files in /kaggle/working/exp14/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp15/labels\n",
      "Updated all label files in /kaggle/working/exp15/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp16/labels\n",
      "Updated all label files in /kaggle/working/exp16/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp17/labels\n",
      "Updated all label files in /kaggle/working/exp17/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp18/labels\n",
      "Updated all label files in /kaggle/working/exp18/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp19/labels\n",
      "Updated all label files in /kaggle/working/exp19/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp20/labels\n",
      "Updated all label files in /kaggle/working/exp20/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp21/labels\n",
      "Updated all label files in /kaggle/working/exp21/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp22/labels\n",
      "Updated all label files in /kaggle/working/exp22/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp23/labels\n",
      "Updated all label files in /kaggle/working/exp23/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp24/labels\n",
      "Updated all label files in /kaggle/working/exp24/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp25/labels\n",
      "Updated all label files in /kaggle/working/exp25/labels with trend column.\n",
      "Processing label directory: /kaggle/working/exp26/labels\n",
      "Updated all label files in /kaggle/working/exp26/labels with trend column.\n",
      "All exp directories processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = \"/kaggle/working\"\n",
    "exp_dirs = sorted(glob.glob(os.path.join(base_dir, \"exp*\")), key=os.path.getmtime)\n",
    "\n",
    "print(f\"Found {len(exp_dirs)} exp directories.\")\n",
    "\n",
    "for exp_dir in exp_dirs:\n",
    "    labels_dir = os.path.join(exp_dir, \"labels\")\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        print(f\"Skipping {exp_dir}, no 'labels' folder found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing label directory: {labels_dir}\")\n",
    "\n",
    "    # Collect bounding box area per track_id per frame\n",
    "    track_areas = defaultdict(list)  # track_id: list of (frame_number, area)\n",
    "    file_list = sorted(glob.glob(os.path.join(labels_dir, \"*.txt\")))\n",
    "    frame_index_map = {file: idx for idx, file in enumerate(file_list)}\n",
    "\n",
    "    for txt_file in file_list:\n",
    "        frame_id = frame_index_map[txt_file]\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "                track_id = int(parts[0])\n",
    "                x1, y1, x2, y2 = map(float, parts[4:8])\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                track_areas[track_id].append((frame_id, area))\n",
    "\n",
    "    # Compute trend per track_id\n",
    "    track_trend = {}\n",
    "    for track_id, entries in track_areas.items():\n",
    "        entries.sort()\n",
    "        frame_ids, areas = zip(*entries)\n",
    "        if len(areas) < 3:\n",
    "            trend = \"stationary\"\n",
    "        else:\n",
    "            slope = np.polyfit(frame_ids, areas, 1)[0]\n",
    "            if slope > 0:\n",
    "                trend = \"approaching\"\n",
    "            elif slope < 0:\n",
    "                trend = \"receding\"\n",
    "            else:\n",
    "                trend = \"stationary\"\n",
    "        track_trend[track_id] = trend\n",
    "\n",
    "    # Rewrite each txt file with trend info as final column\n",
    "    for txt_file in file_list:\n",
    "        new_lines = []\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 8:\n",
    "                    new_lines.append(line)\n",
    "                    continue\n",
    "                track_id = int(parts[0])\n",
    "                trend = track_trend.get(track_id, \"stationary\")\n",
    "                new_line = line.strip() + f\" {trend}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        with open(txt_file, \"w\") as f:\n",
    "            f.writelines(new_lines)\n",
    "\n",
    "    print(f\"Updated all label files in {labels_dir} with trend column.\")\n",
    "\n",
    "print(\"All exp directories processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:51:55.717676Z",
     "iopub.status.busy": "2025-07-12T16:51:55.717443Z",
     "iopub.status.idle": "2025-07-12T16:51:55.722864Z",
     "shell.execute_reply": "2025-07-12T16:51:55.722205Z",
     "shell.execute_reply.started": "2025-07-12T16:51:55.717658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:52:20.372541Z",
     "iopub.status.busy": "2025-07-12T16:52:20.372281Z",
     "iopub.status.idle": "2025-07-12T16:52:20.706719Z",
     "shell.execute_reply": "2025-07-12T16:52:20.706107Z",
     "shell.execute_reply.started": "2025-07-12T16:52:20.372522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission CSV saved to IR.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "submission_rows = []\n",
    "\n",
    "# ---------- PART 1: Image Detection Labels ----------\n",
    "detection_dir = os.path.join(\"yolov5\", \"runs\", \"detect\", \"exp2\", \"labels\")\n",
    "for txt_file in sorted(glob.glob(os.path.join(detection_dir, \"*.txt\"))):\n",
    "    frame_name = os.path.basename(txt_file).replace(\".txt\", \"\")\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 7:\n",
    "                continue\n",
    "            class_id, conf_det, x_min, y_min, x_max, y_max, infer_time = map(float, parts)\n",
    "            class_label = \"bird\" if int(class_id) == 0 else \"drone\"\n",
    "            row = [\n",
    "                frame_name,\n",
    "                0,\n",
    "                x_min, y_min, x_max, y_max,\n",
    "                class_label,\n",
    "                \"0\",\n",
    "                round(conf_det, 2),\n",
    "                round(infer_time, 2),\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0\n",
    "            ]\n",
    "            submission_rows.append(row)\n",
    "\n",
    "# ---------- PART 2: Video Tracking Labels ----------\n",
    "tracking_base_dir = \".\"\n",
    "exp_dirs = sorted([d for d in os.listdir(tracking_base_dir) if d.startswith(\"exp\") and os.path.isdir(os.path.join(tracking_base_dir, d))])\n",
    "\n",
    "for i, exp in enumerate(exp_dirs):\n",
    "    labels_dir = os.path.join(tracking_base_dir, exp, \"labels\")\n",
    "    if not os.path.isdir(labels_dir):\n",
    "        continue\n",
    "\n",
    "    # Determine device by exp index (odd = GPU, even = CPU)\n",
    "    if i % 2 == 0:\n",
    "        continue\n",
    "\n",
    "    for txt_file in sorted(glob.glob(os.path.join(labels_dir, \"*.txt\"))):\n",
    "        frame_name = os.path.basename(txt_file).replace(\".txt\", \"\")\n",
    "        video_name = \"_\".join(frame_name.split(\"_\")[:3])\n",
    "\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 9:\n",
    "                    continue\n",
    "                track_id, class_id, conf_track, conf_det, x_min, y_min, x_max, y_max, direction = parts\n",
    "                class_label = \"bird\" if int(class_id) == 0 else \"drone\"\n",
    "                row = [\n",
    "                    frame_name,\n",
    "                    int(track_id),\n",
    "                    float(x_min), float(y_min), float(x_max), float(y_max),\n",
    "                    class_label,\n",
    "                    direction,\n",
    "                    round(float(conf_det), 2),\n",
    "                    0,\n",
    "                    round(fps_results.get(video_name, {}).get(\"cpu\", 0), 2),\n",
    "                    round(fps_results.get(video_name, {}).get(\"0,1\", 0), 2),\n",
    "                    round(float(conf_track), 2),\n",
    "                    0,\n",
    "                    0\n",
    "                ]\n",
    "                submission_rows.append(row)\n",
    "\n",
    "# ---------- PART 3: Write Final CSV ----------\n",
    "csv_path = \"IR.csv\"\n",
    "header = [\n",
    "    \"Frame_name\", \"track_id\", \"x_min_norm\", \"y_min_norm\", \"x_max_norm\", \"y_max_norm\",\n",
    "    \"class_label\", \"direction\", \"confidence_detection\", \"inference_time_detection (ms)\",\n",
    "    \"FPS (CPU)\", \"FPS (GPU)\", \"confidence_track\", \"payload_label\", \"prob_harmful\"\n",
    "]\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(submission_rows)\n",
    "\n",
    "print(f\"Submission CSV saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T16:53:06.742046Z",
     "iopub.status.busy": "2025-07-12T16:53:06.741506Z",
     "iopub.status.idle": "2025-07-12T16:53:07.324056Z",
     "shell.execute_reply": "2025-07-12T16:53:07.323489Z",
     "shell.execute_reply.started": "2025-07-12T16:53:06.742019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the two CSV files\n",
    "csv_file1 = \"RGB.csv\"\n",
    "csv_file2 = \"IR.csv\"\n",
    "\n",
    "# Read both CSV files\n",
    "df1 = pd.read_csv(csv_file1)\n",
    "df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "# Concatenate them\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "merged_df.to_csv(submission_filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7749164,
     "sourceId": 12294869,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 398789,
     "modelInstanceId": 378651,
     "sourceId": 469353,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
